{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument \n",
    "from gensim.models import Doc2Vec\n",
    "import random\n",
    "import os\n",
    "import nltk.stem as stem\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import imblearn\n",
    "\n",
    "\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = './data/dataset/ebay2gg_root_origin'\n",
    "origin_data_tmp = [os.path.join(root_path,i) for i in os.listdir(os.path.join(root_path))]\n",
    "origin_data=[]\n",
    "for i in origin_data_tmp:\n",
    "    if 'new' not in i and 'check' not in i:\n",
    "        origin_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/dataset/ebay2gg_root_origin/Office Supplies.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Apparel & Accessories.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Vehicles & Parts.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Arts & Entertainment.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Food, Beverages & Tobacco.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Software.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Baby & Toddler.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Business & Industrial.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Home & Garden.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Luggage & Bags.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Health & Beauty.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Religious & Ceremonial.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Sporting Goods.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Hardware.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Media.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Electronics.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Furniture.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Animals & Pet Supplies.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Mature.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Cameras & Optics.txt',\n",
       " './data/dataset/ebay2gg_root_origin/Toys & Games.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in [20,18,14,12,8,5,4,1]:\n",
    "#     print(origin_data[i])\n",
    "#     origin_data.remove(origin_data[i])\n",
    "    \n",
    "# origin_data=origin_data[1:]\n",
    "origin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "label=0\n",
    "for i in origin_data:\n",
    "    if i==\"./data/dataset/.ipynb_checkpoints\":\n",
    "        continue\n",
    "    data = open(i).readlines()\n",
    "    #print(len(data))\n",
    "    for j in data:\n",
    "        X.append(j)\n",
    "        y.append(label)\n",
    "    label+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train  , y_test = train_test_split(X,y,test_size=0.3,stratify =y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training class distributions summary: Counter({3: 6841, 1: 4068, 15: 3109, '\n",
      " '12: 2536, 7: 2482, 8: 2299, 20: 1605, 10: 1173, 19: 428, 14: 325, 17: 300, '\n",
      " '5: 245, 0: 226, 13: 224, 2: 136, 4: 126, 6: 99, 16: 62, 9: 36, 18: 24, 11: '\n",
      " '22})')\n",
      "('Test class distributions summary: Counter({3: 2932, 1: 1744, 15: 1333, 12: '\n",
      " '1087, 7: 1064, 8: 986, 20: 688, 10: 502, 19: 184, 14: 139, 17: 129, 5: 105, '\n",
      " '0: 97, 13: 96, 2: 58, 4: 54, 6: 42, 16: 26, 9: 15, 18: 10, 11: 9})')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint('Training class distributions summary: {}'.format(Counter(y_train)))\n",
    "pprint('Test class distributions summary: {}'.format(Counter(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.95      0.86      1.00      0.90      0.98      0.95        97\n",
      "          1       0.97      0.98      1.00      0.98      0.98      0.97      1744\n",
      "          2       0.73      0.64      1.00      0.68      0.85      0.70        58\n",
      "          3       0.92      0.95      0.97      0.94      0.95      0.90      2932\n",
      "          4       0.71      0.65      1.00      0.68      0.84      0.69        54\n",
      "          5       0.80      0.78      1.00      0.79      0.90      0.79       105\n",
      "          6       0.81      0.93      1.00      0.87      0.90      0.80        42\n",
      "          7       0.89      0.88      0.99      0.89      0.94      0.88      1064\n",
      "          8       0.91      0.90      0.99      0.91      0.95      0.90       986\n",
      "          9       0.75      0.60      1.00      0.67      0.87      0.73        15\n",
      "         10       1.00      0.97      1.00      0.98      1.00      0.99       502\n",
      "         11       1.00      0.44      1.00      0.62      1.00      1.00         9\n",
      "         12       0.97      0.97      1.00      0.97      0.98      0.96      1087\n",
      "         13       0.88      0.69      1.00      0.77      0.94      0.87        96\n",
      "         14       0.89      0.86      1.00      0.88      0.94      0.88       139\n",
      "         15       0.96      0.95      0.99      0.95      0.97      0.95      1333\n",
      "         16       0.81      0.81      1.00      0.81      0.90      0.79        26\n",
      "         17       0.94      0.91      1.00      0.92      0.97      0.94       129\n",
      "         18       0.89      0.80      1.00      0.84      0.94      0.88        10\n",
      "         19       0.92      0.98      1.00      0.95      0.96      0.91       184\n",
      "         20       0.95      0.93      1.00      0.94      0.97      0.94       688\n",
      "\n",
      "avg / total       0.94      0.94      0.99      0.94      0.96      0.92     11300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe1 = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20,n_jobs=8))\n",
    "pipe1.fit(X_train, y_train)\n",
    "y_pred = pipe1.predict(X_test)\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.95      0.87      1.00      0.91      0.98      0.95        97\n",
      "          1       0.98      0.97      1.00      0.97      0.99      0.97      1744\n",
      "          2       0.61      0.71      1.00      0.66      0.78      0.59        58\n",
      "          3       0.92      0.94      0.97      0.93      0.95      0.90      2932\n",
      "          4       0.54      0.65      1.00      0.59      0.73      0.51        54\n",
      "          5       0.84      0.81      1.00      0.83      0.92      0.83       105\n",
      "          6       0.82      0.86      1.00      0.84      0.90      0.80        42\n",
      "          7       0.89      0.85      0.99      0.87      0.93      0.86      1064\n",
      "          8       0.90      0.91      0.99      0.90      0.94      0.88       986\n",
      "          9       0.75      0.60      1.00      0.67      0.87      0.73        15\n",
      "         10       0.98      0.97      1.00      0.97      0.99      0.97       502\n",
      "         11       1.00      0.44      1.00      0.62      1.00      1.00         9\n",
      "         12       0.95      0.96      1.00      0.96      0.98      0.95      1087\n",
      "         13       0.86      0.72      1.00      0.78      0.93      0.85        96\n",
      "         14       0.84      0.93      1.00      0.88      0.92      0.83       139\n",
      "         15       0.95      0.94      0.99      0.95      0.97      0.94      1333\n",
      "         16       0.83      0.73      1.00      0.78      0.91      0.81        26\n",
      "         17       0.96      0.91      1.00      0.94      0.98      0.95       129\n",
      "         18       0.86      0.60      1.00      0.71      0.93      0.84        10\n",
      "         19       0.90      0.96      1.00      0.93      0.95      0.89       184\n",
      "         20       0.94      0.94      1.00      0.94      0.97      0.93       688\n",
      "\n",
      "avg / total       0.93      0.93      0.99      0.93      0.96      0.91     11300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use smote to handle samples imbalanced problem\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pipe = make_pipeline_imb(TfidfVectorizer(),\n",
    "                         #RandomUnderSampler(),\n",
    "                         SMOTE(random_state=0,n_jobs=8),\n",
    "                         RandomForestClassifier(n_jobs=8))\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report_imbalanced(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset/ebay2gg_root_origin/Arts & Entertainment.txt\n"
     ]
    }
   ],
   "source": [
    "new_x_test = ['Musical Instruments & Gear:Instruction Books, CDs & Video:Strings']\n",
    "pred_label = pipe1.predict(new_x_test)\n",
    "for i in pred_label:\n",
    "    print(origin_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tf_idf on one root category\n",
    "import pandas as pd\n",
    "import nltk.stem as stem\n",
    "s=stem.SnowballStemmer('english')\n",
    "\n",
    "\n",
    "df3 = pd.read_csv('./data/gpc_id2name.tsv',sep='\\t')\n",
    "a=df3[[\"GPC_NAME\"]].values\n",
    "b=[[i[0],i[0]]    for i in a]\n",
    "for i in range(len(b)):\n",
    "    b[i][1] =  b[i][1].split(\">\")[0].strip()\n",
    "    \n",
    "df_101 = pd.DataFrame(b,columns=['gg_categ',\"gg_first_categ\"])\n",
    "train_data = df_101.query(\"gg_first_categ=='Arts & Entertainment'\")\n",
    "train_data=train_data[['gg_categ']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 487\n",
      "['art', 'entertain', 'hobbi', 'creativ', 'art', 'art', 'craft', 'art', 'craft', 'tool', 'frame', ',', 'hoop', 'stretcher']\n",
      "['art', 'entertain', 'hobbi', 'creativ', 'art', 'art', 'craft', 'art', 'craft', 'materi', 'textil', 'craft', 'canva']\n",
      "['art', 'entertain', 'hobbi', 'creativ', 'art', 'art', 'craft', 'craft', 'organ', 'thread', 'yarn', 'organ']\n",
      "['art', 'entertain', 'hobbi', 'creativ', 'art', 'collect', 'autograph']\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "new_arr = train_data.reshape(len(train_data))\n",
    "new_list = list(new_arr)\n",
    "new_new_list = [i.replace(\">\",\" \").replace(\"&\",\" \") for i in new_list]\n",
    "print(\"Number of documents:\",len(new_new_list))\n",
    "gen_docs = [[s.stem(w.lower()) for w in word_tokenize(text)] \n",
    "            for text in new_new_list]\n",
    "print(gen_docs[131])\n",
    "print(gen_docs[84])\n",
    "print(gen_docs[161])\n",
    "print(gen_docs[169])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(len(dictionary))\n",
    "sims = gensim.similarities.Similarity('./',tf_idf[corpus],num_features=len(dictionary),num_best=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['music', 'instrument', 'gear', 'instruct', 'book', 'cds', 'video', 'string']\n",
      "[(249, 1), (250, 1), (297, 1)]\n",
      "[(249, 0.2787648854759874), (250, 0.2787648854759874), (297, 0.9190104881072464)]\n"
     ]
    }
   ],
   "source": [
    "#查不到得处理\n",
    "test_sample = new_x_test[0]\n",
    "test_sample1=test_sample.replace(\"&\",\" \").replace(\":\",\" \").replace('(',' ').replace(\")\",\" \").replace(\"/\",\" \").replace(\",\",\" \")\n",
    "query_doc = [s.stem(w.lower()) for w in word_tokenize(test_sample1)]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(412, 0.96910041570663452), (267, 0.77196121215820312), (290, 0.76630705595016479), (291, 0.76401937007904053), (294, 0.7584843635559082)]\n",
      "google_category_1:   Arts & Entertainment > Hobbies & Creative Arts > Musical Instruments > String Instruments\n",
      "google_category_2:   Arts & Entertainment > Hobbies & Creative Arts > Musical Instrument & Orchestra Accessories > String Instrument Accessories\n",
      "google_category_3:   Arts & Entertainment > Hobbies & Creative Arts > Musical Instrument & Orchestra Accessories > String Instrument Accessories > Orchestral String Instrument Accessories > Orchestral String Instrument Strings\n",
      "google_category_4:   Arts & Entertainment > Hobbies & Creative Arts > Musical Instrument & Orchestra Accessories > String Instrument Accessories > String Instrument Care & Cleaning\n",
      "google_category_5:   Arts & Entertainment > Hobbies & Creative Arts > Musical Instrument & Orchestra Accessories > String Instrument Accessories > String Instrument Care & Cleaning > String Instrument Polish\n",
      "ebay_category    :   Musical Instruments & Gear:Instruction Books, CDs & Video:Strings\n"
     ]
    }
   ],
   "source": [
    "res=sims[query_doc_tf_idf]\n",
    "\n",
    "# print(np.argmax(res)+1,len(res))\n",
    "# print(\"google category:   \"+new_list[np.argmax(res)])\n",
    "# print(\"ebay category  :   \"+test_sample)\n",
    "\n",
    "print(res)\n",
    "for i in range(len(res)):\n",
    "    print(\"google_category_\"+str(i+1)+\":   \"+new_list[res[i][0]])           \n",
    "\n",
    "# print(\"google_category_1:   \"+new_list[res[0][0]])\n",
    "# print(\"google_category_2:   \"+new_list[res[1][0]])\n",
    "# print(\"google_category_3:   \"+new_list[res[2][0]])\n",
    "# print(\"google_category_4:   \"+new_list[res[3][0]])\n",
    "# print(\"google_category_5:   \"+new_list[res[4][0]])\n",
    "\n",
    "\n",
    "print(\"ebay_category    :   \"+test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                    Arts & Entertainment > Hobbies & Creative Arts > Collectibles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
