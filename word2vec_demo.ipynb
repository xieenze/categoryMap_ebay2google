{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考：\n",
    "http://blog.csdn.net/lk7688535/article/details/52798735  \n",
    "http://blog.csdn.net/angus_monroe/article/details/76999920  \n",
    "http://blog.csdn.net/Jerr__y/article/details/52967351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['leaf_categ_id', 'leaf_categ_name', 'site_id'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取 google category数据\n",
    "df = pd.read_csv('./data/ares-presto_run_15_stmt_1_0.csv',sep=',')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpc_name=df[['leaf_categ_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gpc_name.to_csv(\"./data/train1.txt\",index =False,header =False,sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences1=word2vec.LineSentence(\"./data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2=word2vec.Text8Corpus(\"./data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LineSentence in module gensim.models.word2vec:\n",
      "\n",
      "class LineSentence(builtins.object)\n",
      " |  Simple format: one sentence = one line; words already preprocessed and separated by whitespace.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      " |      `source` can be either a string or a file object. Clip the file to the first\n",
      " |      `limit` lines (or not clipped if limit is None, the default).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          sentences = LineSentence('myfile.txt')\n",
      " |      \n",
      " |      Or for compressed files::\n",
      " |      \n",
      " |          sentences = LineSentence('compressed_text.txt.bz2')\n",
      " |          sentences = LineSentence('compressed_text.txt.gz')\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the lines in the source.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word2vec.LineSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 科学调包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-10 01:57:21,817 : INFO : collecting all words and their counts\n",
      "2018-01-10 01:57:21,819 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-10 01:57:21,857 : INFO : collected 4181 word types from a corpus of 47517 raw words and 5427 sentences\n",
      "2018-01-10 01:57:21,858 : INFO : Loading a fresh vocabulary\n",
      "2018-01-10 01:57:21,870 : INFO : min_count=1 retains 4181 unique words (100% of original 4181, drops 0)\n",
      "2018-01-10 01:57:21,871 : INFO : min_count=1 leaves 47517 word corpus (100% of original 47517, drops 0)\n",
      "2018-01-10 01:57:21,893 : INFO : deleting the raw counts dictionary of 4181 items\n",
      "2018-01-10 01:57:21,894 : INFO : sample=0.001 downsamples 71 most-common words\n",
      "2018-01-10 01:57:21,895 : INFO : downsampling leaves estimated 33541 word corpus (70.6% of prior 47517)\n",
      "2018-01-10 01:57:21,896 : INFO : estimated required memory for 4181 words and 20 dimensions: 2759460 bytes\n",
      "2018-01-10 01:57:21,909 : INFO : resetting layer weights\n",
      "2018-01-10 01:57:21,996 : INFO : training model with 3 workers on 4181 vocabulary and 20 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-10 01:57:22,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-10 01:57:22,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-10 01:57:22,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-10 01:57:22,375 : INFO : training on 237585 raw words (167735 effective words) took 0.4s, 445179 effective words/s\n",
      "2018-01-10 01:57:22,376 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#model=word2vec.Word2Vec(sentences,min_count=5,size=50)\n",
    "#第一个参数是训练语料，第二个参数是小于该数的单词会被剔除，默认值为5, \n",
    "#第三个参数是神经网络的输出特征维度，默认为100\n",
    "model=word2vec.Word2Vec(sentences1,sg=1,min_count=1,size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#help(gensim.models.Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-10 01:57:22,416 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856916199701\n",
      "[('Fragrance', 0.9785259366035461), ('Mailbox', 0.96456378698349), ('Sink', 0.954559862613678), ('Treatment', 0.9512360692024231), ('Floo', 0.9505041837692261), ('Fla', 0.949422299861908), ('Busines', 0.9494035243988037), ('Gas', 0.9429299831390381), ('Stove', 0.9407431483268738), ('Decor', 0.9403433203697205)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(model.wv.similarity(\"Home\",\"Coin\"))\n",
    "print(model.wv.most_similar(\"Home\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取每个词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Computer' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ea12de6ff73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Computer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Computer' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv['Computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"The fox jumped over a lazy dog\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/\n",
      " |  \n",
      " |  If you're finished training a model (=no more updates, only querying)\n",
      " |  then switch to the :mod:`gensim.models.KeyedVectors` instance in wv\n",
      " |  \n",
      " |  The model can be stored/loaded via its `save()` and `load()` methods, or stored/loaded in a format\n",
      " |  compatible with the original word2vec implementation via `wv.save_word2vec_format()`\n",
      " |  and `KeyedVectors.load_word2vec_format()`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use self.wv.__contains__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__contains__`\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use self.wv.__getitem__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__getitem__`\n",
      " |  \n",
      " |  __init__(self, sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False)\n",
      " |      Initialize the model from an iterable of `sentences`. Each sentence is a\n",
      " |      list of words (unicode strings) that will be used for training.\n",
      " |      \n",
      " |      The `sentences` iterable can be simply a list, but for larger corpora,\n",
      " |      consider an iterable that streams the sentences directly from disk/network.\n",
      " |      See :class:`BrownCorpus`, :class:`Text8Corpus` or :class:`LineSentence` in\n",
      " |      this module for such examples.\n",
      " |      \n",
      " |      If you don't supply `sentences`, the model is left uninitialized -- use if\n",
      " |      you plan to initialize it in some other way.\n",
      " |      \n",
      " |      `sg` defines the training algorithm. By default (`sg=0`), CBOW is used.\n",
      " |      Otherwise (`sg=1`), skip-gram is employed.\n",
      " |      \n",
      " |      `size` is the dimensionality of the feature vectors.\n",
      " |      \n",
      " |      `window` is the maximum distance between the current and predicted word within a sentence.\n",
      " |      \n",
      " |      `alpha` is the initial learning rate (will linearly drop to `min_alpha` as training progresses).\n",
      " |      \n",
      " |      `seed` = for the random number generator. Initial vectors for each\n",
      " |      word are seeded with a hash of the concatenation of word + str(seed).\n",
      " |      Note that for a fully deterministically-reproducible run, you must also limit the model to\n",
      " |      a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python\n",
      " |      3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED\n",
      " |      environment variable to control hash randomization.)\n",
      " |      \n",
      " |      `min_count` = ignore all words with total frequency lower than this.\n",
      " |      \n",
      " |      `max_vocab_size` = limit RAM during vocabulary building; if there are more unique\n",
      " |      words than this, then prune the infrequent ones. Every 10 million word types\n",
      " |      need about 1GB of RAM. Set to `None` for no limit (default).\n",
      " |      \n",
      " |      `sample` = threshold for configuring which higher-frequency words are randomly downsampled;\n",
      " |          default is 1e-3, useful range is (0, 1e-5).\n",
      " |      \n",
      " |      `workers` = use this many worker threads to train the model (=faster training with multicore machines).\n",
      " |      \n",
      " |      `hs` = if 1, hierarchical softmax will be used for model training.\n",
      " |      If set to 0 (default), and `negative` is non-zero, negative sampling will be used.\n",
      " |      \n",
      " |      `negative` = if > 0, negative sampling will be used, the int for negative\n",
      " |      specifies how many \"noise words\" should be drawn (usually between 5-20).\n",
      " |      Default is 5. If set to 0, no negative samping is used.\n",
      " |      \n",
      " |      `cbow_mean` = if 0, use the sum of the context word vectors. If 1 (default), use the mean.\n",
      " |      Only applies when cbow is used.\n",
      " |      \n",
      " |      `hashfxn` = hash function to use to randomly initialize weights, for increased\n",
      " |      training reproducibility. Default is Python's rudimentary built in hash function.\n",
      " |      \n",
      " |      `iter` = number of iterations (epochs) over the corpus. Default is 5.\n",
      " |      \n",
      " |      `trim_rule` = vocabulary trimming rule, specifies whether certain words should remain\n",
      " |      in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |      Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and\n",
      " |      returns either `utils.RULE_DISCARD`, `utils.RULE_KEEP` or `utils.RULE_DEFAULT`.\n",
      " |      Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |      of the model.\n",
      " |      \n",
      " |      `sorted_vocab` = if 1 (default), sort the vocabulary by descending frequency before\n",
      " |      assigning word indexes.\n",
      " |      \n",
      " |      `batch_words` = target size (in words) for batches of examples passed to worker threads (and\n",
      " |      thus cython routines). Default is 10000. (Larger batches will be passed if individual\n",
      " |      texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |  \n",
      " |  build_vocab(self, sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      Build model vocabulary from a passed dictionary that contains (word,word count).\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      `word_freq` : dict\n",
      " |          Word,Word_Count dictionary.\n",
      " |      `keep_raw_vocab` : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      `corpus_count`: int\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      `trim_rule` = vocabulary trimming rule, specifies whether certain words should remain\n",
      " |      in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |      Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and\n",
      " |      returns either `utils.RULE_DISCARD`, `utils.RULE_KEEP` or `utils.RULE_DEFAULT`.\n",
      " |      `update`: bool\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |      \n",
      " |      Returns\n",
      " |      --------\n",
      " |      None\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from gensim.models.word2vec import Word2Vec\n",
      " |      >>> model= Word2Vec()\n",
      " |      >>> model.build_vocab_from_freq({\"Word1\": 15, \"Word2\": 20})\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Removes all L2-normalized vectors for words from the model.\n",
      " |      You will have to recompute them using init_sims method.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a binary Huffman tree using stored vocabulary word counts. Frequent words\n",
      " |      will have shorter binary codes. Called internally from `build_vocab()`.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      " |      If `replace_word_vectors_with_normalized` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated. Use self.wv.doesnt_match() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.doesnt_match`\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated. Use self.wv.evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.evaluate_word_pairs`\n",
      " |  \n",
      " |  finalize_vocab(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      init_sims() resides in KeyedVectors because it deals with syn0 mainly, but because syn1 is not an attribute\n",
      " |      of KeyedVectors, it has to be deleted in this class, and the normalizing of syn0 happens inside of KeyedVectors\n",
      " |  \n",
      " |  initialize_word_vectors(self)\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge the input-hidden weight matrix from the original C word2vec-tool format\n",
      " |      given, where it intersects with the current vocabulary. (No words are added to the\n",
      " |      existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.)\n",
      " |      \n",
      " |      `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
      " |      \n",
      " |      `lockf` is a lock-factor value to be set for any imported word-vectors; the\n",
      " |      default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |      training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |  \n",
      " |  make_cum_table(self, power=0.75, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the\n",
      " |      table (cum_table[-1]), then finding that integer's sorted insertion point\n",
      " |      (as if by bisect_left or ndarray.searchsorted()). That insertion point is the\n",
      " |      drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |      \n",
      " |      Called internally from 'build_vocab()'.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated. Use self.wv.most_similar() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.most_similar`\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated. Use self.wv.most_similar_cosmul() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.most_similar_cosmul`\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated. Use self.wv.n_similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.n_similarity`\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Report the probability distribution of the center word given the context words\n",
      " |      as input to the trained model.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures (like vocab) from the other_model. Useful\n",
      " |      if testing multiple models in parallel on the same corpus.\n",
      " |  \n",
      " |  reset_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the object to file (also see `load`).\n",
      " |      \n",
      " |      `fname_or_handle` is either a string specifying the file name to\n",
      " |      save to, or an open file-like object which can be written to. If\n",
      " |      the object is a file handle, no special array handling will be\n",
      " |      performed; all attributes will be saved to the same file.\n",
      " |      \n",
      " |      If `separately` is None, automatically detect large\n",
      " |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |      them into separate files. This avoids pickle memory errors and\n",
      " |      allows mmap'ing large arrays back on load efficiently.\n",
      " |      \n",
      " |      You can also set `separately` manually, in which case it must be\n",
      " |      a list of attribute names to be stored in separate files. The\n",
      " |      automatic check is not performed in this case.\n",
      " |      \n",
      " |      `ignore` is a set of attribute names to *not* serialize (file\n",
      " |      handles, caches etc). On subsequent load() these attributes will\n",
      " |      be set to None.\n",
      " |      \n",
      " |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      " |      in both Python 2 and 3.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use model.wv.save_word2vec_format instead.\n",
      " |  \n",
      " |  scale_vocab(self, min_count=None, sample=None, dry_run=False, keep_raw_vocab=False, trim_rule=None, update=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  scan_vocab(self, sentences, progress_per=10000, trim_rule=None)\n",
      " |      Do an initial scan of all words appearing in sentences.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |      This does not change the fitted model in any way (see Word2Vec.train() for that).\n",
      " |      \n",
      " |      We have currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with hs=1 and negative=0 for this to work.\n",
      " |      \n",
      " |      Note that you should specify total_sentences; we'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the article by [#taddy]_ and the gensim demo at [#deepir]_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      .. [#taddy] Taddy, Matt.  Document Classification by Inversion of Distributed Language Representations,\n",
      " |                  in Proceedings of the 2015 Conference of the Association of Computational Linguistics.\n",
      " |      .. [#deepir] https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb\n",
      " |  \n",
      " |  seeded_vector(self, seed_string)\n",
      " |      Create one 'random' vector (but deterministic by seed_string)\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_vector() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similar_by_vector`\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_word() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similar_by_word`\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated. Use self.wv.similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similarity`\n",
      " |  \n",
      " |  sort_vocab(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  train(self, sentences, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=None)\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      For Word2Vec, each sentence must be a list of unicode strings. (Subclasses may accept other examples.)\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate\n",
      " |      progres-percentage logging, either total_examples (count of sentences) or total_words (count of\n",
      " |      raw words in sentences) MUST be provided. (If the corpus is the same as was provided to\n",
      " |      `build_vocab()`, the count of examples in that corpus will be available in the model's\n",
      " |      `corpus_count` property.)\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument MUST be provided. In the common and recommended case, where `train()`\n",
      " |      is only called once, the model's cached `iter` value should be supplied as `epochs` value.\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly\n",
      " |      added vocabulary.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated. Use self.wv.wmdistance() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.wmdistance`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |      Deprecated. Use self.wv.log_evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.log_evaluate_word_pairs`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.models.Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
