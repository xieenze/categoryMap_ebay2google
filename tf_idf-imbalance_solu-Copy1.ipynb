{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import imblearn\n",
    "\n",
    "import nltk.stem as stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(rootpath):\n",
    "    '''\n",
    "    input : rootpath(str) 是 google root category的路径\n",
    "    return : X_train,X_test,y_train,y_test. 21个类别的数据，70%训练，30%测试\n",
    "    '''\n",
    "    origin_data_tmp = [os.path.join(root_path,i) for i in os.listdir(os.path.join(root_path))]\n",
    "    origin_data=[]\n",
    "    for i in origin_data_tmp:\n",
    "        if 'new' not in i and 'check' not in i:\n",
    "            origin_data.append(i)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    label=0\n",
    "    for i in origin_data:\n",
    "        if i==\"./data/dataset/.ipynb_checkpoints\":\n",
    "            continue\n",
    "        data = open(i).readlines()\n",
    "        #print(len(data))\n",
    "        for j in data:\n",
    "            X.append(j)\n",
    "            y.append(label)\n",
    "        label+=1\n",
    "    X_train , X_test , y_train  , y_test = train_test_split(X,y,test_size=0.3,stratify =y) \n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "\n",
    "def get_class_map(rootpath):\n",
    "    '''\n",
    "    input : rootpath(str) 是 google root category的路径\n",
    "    return : 一个字典， key是类别号(0-20),value 是 对应的google root category的名字\n",
    "    '''\n",
    "    origin_data_tmp = [i for i in os.listdir(os.path.join(root_path))]\n",
    "    origin_data=[]\n",
    "    for i in origin_data_tmp:\n",
    "        if 'new' not in i and 'check' not in i:\n",
    "            origin_data.append(i)\n",
    "    class_dict = dict([[i,origin_data[i].split('.')[0]] for i in range(len(origin_data))])\n",
    "    return class_dict\n",
    "\n",
    "\n",
    "\n",
    "class tfidf_rf_pipe(object):\n",
    "    '''\n",
    "    训练一个pipeline，可以将ebay category分类到21个google root category中的一个。\n",
    "    1.先用TF-IDF将一条category转化成稀疏的vector\n",
    "    2.用random forest 分类模型进行分类。\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        定义pipeline\n",
    "        '''\n",
    "        self.pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20,n_jobs=8))\n",
    "    \n",
    "    def train(self,X_train,y_train):\n",
    "        '''\n",
    "        训练模型\n",
    "        输入: X_train,y_train(list)\n",
    "        '''\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        print('train complete!')\n",
    "    \n",
    "    def validation(self,X_val,y_val):\n",
    "        '''\n",
    "        验证模型的准确度\n",
    "        输入:X_val,y_val(list)\n",
    "        返回:模型的各项指标，如准确率，召回率，特异性，几何平均值和指标平衡准确度\n",
    "        '''\n",
    "        y_pred = self.pipeline.predict(X_val)\n",
    "        res = classification_report_imbalanced(y_val, y_pred,target_names=[class_map[i] for i in class_map])\n",
    "        return res\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        '''\n",
    "        预测新的ebay category属于哪一类google first category.\n",
    "        输入:X_test(list)\n",
    "        返回:对应的类别\n",
    "        '''\n",
    "        return self.pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "rootpath下包括21个google root category对应的文件，每个文件中存放所有对应的ebay category\n",
    "'''\n",
    "\n",
    "root_path = './data/dataset/ebay2gg_root_origin'\n",
    "X_train,X_test,y_train,y_test = get_train_test(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pprint('Training class distributions summary: ')\n",
    "# pprint((sorted(Counter(y_train).items())))\n",
    "# pprint('Test class distributions summary: ')\n",
    "# pprint((sorted(Counter(y_test).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(classification_report_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Office Supplies',\n",
      " 1: 'Food, Beverages & Tobacco',\n",
      " 2: 'Home & Garden',\n",
      " 3: 'Cameras & Optics',\n",
      " 4: 'Religious & Ceremonial',\n",
      " 5: 'Apparel & Accessories',\n",
      " 6: 'Mature',\n",
      " 7: 'Business & Industrial',\n",
      " 8: 'Sporting Goods',\n",
      " 9: 'Hardware',\n",
      " 10: 'Baby & Toddler',\n",
      " 11: 'Media',\n",
      " 12: 'Animals & Pet Supplies',\n",
      " 13: 'Arts & Entertainment',\n",
      " 14: 'Luggage & Bags',\n",
      " 15: 'Software',\n",
      " 16: 'Furniture',\n",
      " 17: 'Toys & Games',\n",
      " 18: 'Electronics',\n",
      " 19: 'Vehicles & Parts',\n",
      " 20: 'Health & Beauty'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map = get_class_map(root_path)\n",
    "pprint(class_map)\n",
    "tmp=[class_map[i] for i in class_map]\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete!\n",
      "                                 pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          Office Supplies       0.95      0.79      1.00      0.87      0.97      0.94        97\n",
      "Food, Beverages & Tobacco       0.80      0.81      1.00      0.81      0.89      0.78        54\n",
      "            Home & Garden       0.90      0.89      0.99      0.89      0.94      0.88       986\n",
      "         Cameras & Optics       0.92      0.98      1.00      0.95      0.96      0.92       184\n",
      "   Religious & Ceremonial       1.00      0.44      1.00      0.62      1.00      1.00         9\n",
      "    Apparel & Accessories       0.98      0.97      1.00      0.98      0.99      0.97      1744\n",
      "                   Mature       0.89      0.80      1.00      0.84      0.94      0.88        10\n",
      "    Business & Industrial       0.90      0.87      0.99      0.88      0.94      0.88      1064\n",
      "           Sporting Goods       0.95      0.95      1.00      0.95      0.97      0.95      1087\n",
      "                 Hardware       0.82      0.66      1.00      0.73      0.90      0.80        96\n",
      "           Baby & Toddler       0.90      0.88      1.00      0.89      0.95      0.89        42\n",
      "                    Media       0.84      0.96      1.00      0.89      0.91      0.82       139\n",
      "   Animals & Pet Supplies       0.94      0.91      1.00      0.92      0.97      0.93       129\n",
      "     Arts & Entertainment       0.92      0.95      0.97      0.94      0.95      0.90      2932\n",
      "           Luggage & Bags       0.86      0.40      1.00      0.55      0.93      0.84        15\n",
      "                 Software       0.84      0.85      1.00      0.84      0.92      0.83       105\n",
      "                Furniture       0.89      0.65      1.00      0.76      0.95      0.88        26\n",
      "             Toys & Games       0.95      0.95      1.00      0.95      0.97      0.94       688\n",
      "              Electronics       0.93      0.96      0.99      0.95      0.96      0.92      1333\n",
      "         Vehicles & Parts       0.65      0.53      1.00      0.58      0.80      0.62        58\n",
      "          Health & Beauty       0.97      0.96      1.00      0.97      0.99      0.97       502\n",
      "\n",
      "              avg / total       0.93      0.93      0.99      0.93      0.96      0.92     11300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=tfidf_rf_pipe()\n",
    "model.train(X_train,y_train)\n",
    "res=model.validation(X_test,y_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_x_test = ['Pottery & Glass:Pottery & China:China & Dinnerware:Meakin J. & G.',\n",
    "#              'Sporting Goods:Outdoor Sports:Climbing & Caving:Books & Video']\n",
    "\n",
    "# pred_label = model.predict(new_x_test)\n",
    "# for i in pred_label:\n",
    "#     print(class_map[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class similar_model(object):\n",
    "    def __init__(self,gpc_id2name_path):\n",
    "        self.gpc_id2name_path = gpc_id2name_path\n",
    "        self.s = stem.SnowballStemmer('english')\n",
    "        self.sims = None\n",
    "        self.tf_idf =None\n",
    "        self.dictionary = None\n",
    "        self.new_list = None\n",
    "    def  train_tfidf(self,cate_class):\n",
    "        df3 = pd.read_csv(self.gpc_id2name_path,sep='\\t')\n",
    "        a=df3[[\"GPC_NAME\"]].values\n",
    "        b=[[i[0], i[0].split(\">\")[0].strip()] for i in a]\n",
    "\n",
    "        df_101 = pd.DataFrame(b,columns=['gg_categ',\"gg_first_categ\"])\n",
    "\n",
    "        train_data = df_101.query(\"gg_first_categ=='{}'\".format(class_map[cate_class]))\n",
    "        train_data = train_data[['gg_categ']].values\n",
    "        new_arr = train_data.reshape(len(train_data))\n",
    "        self.new_list = list(new_arr)\n",
    "        new_new_list = [i.replace(\">\",\" \").replace(\"&\",\" \") for i in self.new_list]\n",
    "        #print(\"Number of documents:\",len(new_new_list))\n",
    "        gen_docs = [[self.s.stem(w.lower()) for w in word_tokenize(text)] \n",
    "                    for text in new_new_list]\n",
    "        #print(gen_docs[:10])\n",
    "        self.dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "        corpus = [self.dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "        self.tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        \n",
    "        #self.sims = gensim.similarities.Similarity('./',self.tf_idf[corpus],num_features=len(self.dictionary),num_best=5)\n",
    "        self.sims = gensim.similarities.Similarity('./',corpus,num_features=len(self.dictionary),num_best=5)\n",
    "        print('train complete!')\n",
    "        \n",
    "    def predict(self,sentense):\n",
    "        #查不到得处理,匹配第一个\n",
    "        \n",
    "        test_sample1=sentense.replace(\"&\",\" \").replace(\":\",\" \").replace('(',' ').replace(\")\",\" \").replace(\"/\",\" \").replace(\",\",\" \").replace(\"-\",\" \")\n",
    "        query_doc = [self.s.stem(w.lower()) for w in word_tokenize(test_sample1)]\n",
    "        #print(query_doc)\n",
    "        query_doc_bow = self.dictionary.doc2bow(query_doc)\n",
    "        #print(query_doc_bow)\n",
    "        query_doc_tf_idf = self.tf_idf[query_doc_bow]\n",
    "        #print(query_doc_tf_idf)\n",
    "        restmp1=self.sims[query_doc_tf_idf]\n",
    "        #restmp2 = np.array(res,dtype=\"int64\")[:,0:1].reshape(len(res))\n",
    "        if len(restmp1)==0:\n",
    "            top_5_result = [self.new_list[0]]\n",
    "        else :\n",
    "            top_5_result = [self.new_list[restmp1[i][0]] for i in range(len(restmp1))]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"ebay_category    :   \"+ sentense)\n",
    "#         for i in range(len(top_5_result)):\n",
    "#             print(\"google_category_\" + str(i+1) + \":   \"+ top_5_result[i]) \n",
    "            \n",
    "        return top_5_result\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gpc_id2name_path = './data/gpc_id2name.tsv'\n",
    "\n",
    "# model2 = similar_model(gpc_id2name_path)\n",
    "# model2.train_tfidf(pred_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top5_res= model2.predict(new_x_test[0])\n",
    "# top5_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load ebat alive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16996\n"
     ]
    }
   ],
   "source": [
    "#读取 ebay category数据,获得 us 数据\n",
    "df = pd.read_csv('./data/ares-presto_run_4_stmt_1_0.csv',sep=',')\n",
    "df = df[df[\"site_id\"] == 0]\n",
    "df=df.query(\"leaf_categ_id==move_to\")\n",
    "print(len(df))\n",
    "test = df[['leaf_categ_name']].values\n",
    "test =  test.reshape(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Business & Industrial:Healthcare, Lab & Life Science:Medical Specialties:Emergency & EMT:Training & Manikins',\n",
       "       'Business & Industrial:Healthcare, Lab & Life Science:Medical Specialties:Emergency & EMT:Other Emergency & EMT',\n",
       "       'Business & Industrial:Healthcare, Lab & Life Science:Lab Supplies:Lab Kits & Sets',\n",
       "       'Business & Industrial:Healthcare, Lab & Life Science:Lab Supplies:Plasticware',\n",
       "       'Toys & Hobbies:Radio Control & Control Line:RC Model Vehicle Parts & Accs:Engine, Exhaust & Fuel Systems:Electric Motors',\n",
       "       'Pottery & Glass:Glass:Glassware:Contemporary Glass:Degenhart',\n",
       "       'Business & Industrial:Electrical & Test Equipment:Connectors, Switches & Wire:Wire & Cable:Telecom Wire & Cable',\n",
       "       'Business & Industrial:Electrical & Test Equipment:Connectors, Switches & Wire:Wire & Cable:Magnet/Enameled Wire',\n",
       "       'Business & Industrial:Electrical & Test Equipment:Electrical Supply Equipment:Electr. Supply Books & Manuals',\n",
       "       'Business & Industrial:Electrical & Test Equipment:Electrical Supply Equipment:Other Electr. Supply Equipment',\n",
       "       'Pottery & Glass:Glass:Glassware:Contemporary Glass:Mosser',\n",
       "       'Collectibles:Clocks:Vintage (1930-69):Cuckoo, Black Forest',\n",
       "       'Collectibles:Clocks:Modern (1970-Now):Shelf, Mantel',\n",
       "       'Baby:Bathing & Grooming:Bathing Accessories',\n",
       "       'Baby:Diapering:Swim Diapers', 'Baby:Baby Gear:Other Baby Gear',\n",
       "       'Baby:Toys for Baby:Blocks & Sorters',\n",
       "       'Baby:Toys for Baby:Crib Toys',\n",
       "       'Baby:Toys for Baby:Developmental Baby Toys',\n",
       "       'Collectibles:Clocks:Modern (1970-Now):Wall',\n",
       "       'Sporting Goods:Outdoor Sports:Camping & Hiking:Camping Knives & Tools:Other Camping Knives & Tools',\n",
       "       'Sporting Goods:Cycling:Bicycle Components & Parts:Brake Levers',\n",
       "       'Sporting Goods:Outdoor Sports:Equestrian:Riding Boots & Accessories:Paddock & Jodhpur Boots',\n",
       "       'Home & Garden:Yard, Garden & Outdoor Living:Garden Décor:Sundials',\n",
       "       'Collectibles:Clocks:Parts & Tools:Other Clock Parts & Tools',\n",
       "       'Collectibles:Clocks:Parts & Tools:Attachments',\n",
       "       'Collectibles:Clocks:Parts & Tools:Hand Tools',\n",
       "       'Collectibles:Clocks:Parts & Tools:Machinery',\n",
       "       'Collectibles:Clocks:Parts & Tools:Parts',\n",
       "       'Pet Supplies:Fish & Aquariums:CO2 Equipment'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "test[i:i+30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete!\n"
     ]
    }
   ],
   "source": [
    "index = 888\n",
    "\n",
    "pred_label = model.predict(test[index:index+1])\n",
    "\n",
    "\n",
    "gpc_id2name_path = './data/gpc_id2name.tsv'\n",
    "model2 = similar_model(gpc_id2name_path)\n",
    "model2.train_tfidf(pred_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collectibles:Vintage, Retro, Mid-Century:Plastic\n",
      "\n",
      "Home & Garden > Kitchen & Dining > Food Storage > Food Wraps > Plastic Wrap\n"
     ]
    }
   ],
   "source": [
    "top5_res = model2.predict(test[index])\n",
    "print(test[index])\n",
    "print()\n",
    "for i in top5_res:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
