{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import imblearn\n",
    "\n",
    "import nltk.stem as stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = './data/dataset/ebay2gg_root_origin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(rootpath):\n",
    "    origin_data_tmp = [os.path.join(root_path,i) for i in os.listdir(os.path.join(root_path))]\n",
    "    origin_data=[]\n",
    "    for i in origin_data_tmp:\n",
    "        if 'new' not in i and 'check' not in i:\n",
    "            origin_data.append(i)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    label=0\n",
    "    for i in origin_data:\n",
    "        if i==\"./data/dataset/.ipynb_checkpoints\":\n",
    "            continue\n",
    "        data = open(i).readlines()\n",
    "        #print(len(data))\n",
    "        for j in data:\n",
    "            X.append(j)\n",
    "            y.append(label)\n",
    "        label+=1\n",
    "    X_train , X_test , y_train  , y_test = train_test_split(X,y,test_size=0.3,stratify =y) \n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_class_map(rootpath):\n",
    "    origin_data_tmp = [i for i in os.listdir(os.path.join(root_path))]\n",
    "    origin_data=[]\n",
    "    for i in origin_data_tmp:\n",
    "        if 'new' not in i and 'check' not in i:\n",
    "            origin_data.append(i)\n",
    "    class_dict = dict([[i,origin_data[i].split('.')[0]] for i in range(len(origin_data))])\n",
    "    return class_dict\n",
    "\n",
    "\n",
    "\n",
    "class tfidf_rf_pipe(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20,n_jobs=8))\n",
    "    \n",
    "    def train(self,X_train,y_train):\n",
    "        '''\n",
    "        注释\n",
    "        '''\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        print('train complete!')\n",
    "    \n",
    "    def validation(self,X_val,y_val):\n",
    "        y_pred = self.pipeline.predict(X_val)\n",
    "        res = classification_report_imbalanced(y_val, y_pred)\n",
    "        return res\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return self.pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train , X_test , y_train  , y_test = get_train_test(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module __main__:\n",
      "\n",
      "train(self, X_train, y_train)\n",
      "    注释\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pprint('Training class distributions summary: ')\n",
    "# pprint((sorted(Counter(y_train).items())))\n",
    "# pprint('Test class distributions summary: ')\n",
    "# pprint((sorted(Counter(y_test).items())))\n",
    "help(tfidf_rf_pipe.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete!\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.98      0.90      1.00      0.94      0.99      0.97        97\n",
      "          1       0.98      0.97      1.00      0.97      0.99      0.98      1744\n",
      "          2       0.85      0.60      1.00      0.71      0.92      0.84        58\n",
      "          3       0.92      0.95      0.97      0.93      0.95      0.90      2932\n",
      "          4       0.74      0.65      1.00      0.69      0.86      0.72        54\n",
      "          5       0.87      0.81      1.00      0.84      0.93      0.85       105\n",
      "          6       0.79      0.74      1.00      0.77      0.89      0.78        42\n",
      "          7       0.90      0.87      0.99      0.89      0.94      0.88      1064\n",
      "          8       0.92      0.89      0.99      0.91      0.96      0.91       986\n",
      "          9       0.58      0.47      1.00      0.52      0.76      0.56        15\n",
      "         10       0.98      0.97      1.00      0.97      0.99      0.97       502\n",
      "         11       1.00      0.44      1.00      0.62      1.00      1.00         9\n",
      "         12       0.96      0.96      1.00      0.96      0.98      0.95      1087\n",
      "         13       0.75      0.66      1.00      0.70      0.86      0.73        96\n",
      "         14       0.84      0.92      1.00      0.88      0.92      0.83       139\n",
      "         15       0.95      0.97      0.99      0.96      0.97      0.94      1333\n",
      "         16       0.87      0.77      1.00      0.82      0.93      0.86        26\n",
      "         17       0.95      0.91      1.00      0.93      0.98      0.95       129\n",
      "         18       1.00      0.70      1.00      0.82      1.00      1.00        10\n",
      "         19       0.93      0.99      1.00      0.96      0.97      0.93       184\n",
      "         20       0.93      0.94      1.00      0.94      0.97      0.93       688\n",
      "\n",
      "avg / total       0.93      0.93      0.99      0.93      0.96      0.92     11300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=tfidf_rf_pipe()\n",
    "model.train(X_train,y_train)\n",
    "res=model.validation(X_test,y_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test = ['Pottery & Glass:Pottery & China:China & Dinnerware:Meakin J. & G.',\n",
    "             'Home & Garden:Tools:Power Tools:Buffers & Polishers']\n",
    "class_map = get_class_map(root_path)\n",
    "pred_label = model.predict(new_x_test)\n",
    "# for i in pred_label:\n",
    "#     print(class_map[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class similar_model(object):\n",
    "    def __init__(self,gpc_id2name_path):\n",
    "        self.gpc_id2name_path = gpc_id2name_path\n",
    "        self.s = stem.SnowballStemmer('english')\n",
    "        self.sims = None\n",
    "        self.tf_idf =None\n",
    "        self.dictionary = None\n",
    "        self.new_list = None\n",
    "    def  train_tfidf(self,cate_class):\n",
    "        df3 = pd.read_csv(self.gpc_id2name_path,sep='\\t')\n",
    "        a=df3[[\"GPC_NAME\"]].values\n",
    "        b=[[i[0], i[0].split(\">\")[0].strip()] for i in a]\n",
    "\n",
    "        df_101 = pd.DataFrame(b,columns=['gg_categ',\"gg_first_categ\"])\n",
    "\n",
    "        train_data = df_101.query(\"gg_first_categ=='{}'\".format(class_map[cate_class]))\n",
    "        train_data = train_data[['gg_categ']].values\n",
    "        new_arr = train_data.reshape(len(train_data))\n",
    "        self.new_list = list(new_arr)\n",
    "        new_new_list = [i.replace(\">\",\" \").replace(\"&\",\" \") for i in self.new_list]\n",
    "        #print(\"Number of documents:\",len(new_new_list))\n",
    "        gen_docs = [[self.s.stem(w.lower()) for w in word_tokenize(text)] \n",
    "                    for text in new_new_list]\n",
    "        #print(gen_docs[:10])\n",
    "        self.dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "        corpus = [self.dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "        self.tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        \n",
    "        #self.sims = gensim.similarities.Similarity('./',self.tf_idf[corpus],num_features=len(self.dictionary),num_best=5)\n",
    "        self.sims = gensim.similarities.Similarity('./',corpus,num_features=len(self.dictionary),num_best=5,norm='l2')\n",
    "        #print('train complete!')\n",
    "        \n",
    "    def predict(self,sentense):\n",
    "        \n",
    "        test_sample1=sentense.replace(\"&\",\" \").replace(\":\",\" \").replace('(',' ').replace(\")\",\" \").replace(\"/\",\" \").replace(\",\",\" \").replace(\"-\",\" \")\n",
    "        query_doc = [self.s.stem(w.lower()) for w in word_tokenize(test_sample1)]\n",
    "        #print(query_doc)\n",
    "        query_doc_bow = self.dictionary.doc2bow(query_doc)\n",
    "        #print(query_doc_bow)\n",
    "        query_doc_tf_idf = self.tf_idf[query_doc_bow]\n",
    "        #print(query_doc_tf_idf)\n",
    "        restmp1=self.sims[query_doc_tf_idf]\n",
    "        #restmp2 = np.array(res,dtype=\"int64\")[:,0:1].reshape(len(res))\n",
    "        #查不到得处理,匹配最短的\n",
    "        if len(restmp1)==0:\n",
    "            top_5_result = [self.new_list[0]]\n",
    "        else :\n",
    "            top_5_result = [self.new_list[restmp1[i][0]] for i in range(len(restmp1))]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"ebay_category    :   \"+ sentense)\n",
    "#         for i in range(len(top_5_result)):\n",
    "#             print(\"google_category_\" + str(i+1) + \":   \"+ top_5_result[i]) \n",
    "            \n",
    "        return top_5_result\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Similarity in module gensim.similarities.docsim:\n",
      "\n",
      "class Similarity(gensim.interfaces.SimilarityABC)\n",
      " |  Compute cosine similarity of a dynamic query against a static corpus of documents\n",
      " |  (\"the index\").\n",
      " |  \n",
      " |  Scalability is achieved by sharding the index into smaller pieces, each of which\n",
      " |  fits into core memory (see the `(Sparse)MatrixSimilarity` classes in this module).\n",
      " |  The shards themselves are simply stored as files to disk and mmap'ed back as needed.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Similarity\n",
      " |      gensim.interfaces.SimilarityABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, query)\n",
      " |      Get similarities of document `query` to all documents in the corpus.\n",
      " |      \n",
      " |      **or**\n",
      " |      \n",
      " |      If `query` is a corpus (iterable of documents), return a matrix of similarities\n",
      " |      of all query documents vs. all corpus document. This batch query is more\n",
      " |      efficient than computing the similarities one document after another.\n",
      " |  \n",
      " |  __init__(self, output_prefix, corpus, num_features, num_best=None, chunksize=256, shardsize=32768, norm='l2')\n",
      " |      Construct the index from `corpus`. The index can be later extended by calling\n",
      " |      the `add_documents` method. **Note**: documents are split (internally, transparently)\n",
      " |      into shards of `shardsize` documents each, converted to a matrix, for faster BLAS calls.\n",
      " |      Each shard is stored to disk under `output_prefix.shard_number` (=you need write\n",
      " |      access to that location). If you don't specify an output prefix, a random\n",
      " |      filename in temp will be used.\n",
      " |      \n",
      " |      `shardsize` should be chosen so that a `shardsize x chunksize` matrix of floats\n",
      " |      fits comfortably into main memory.\n",
      " |      \n",
      " |      `num_features` is the number of features in the `corpus` (e.g. size of the\n",
      " |      dictionary, or the number of latent topics for latent semantic models).\n",
      " |      \n",
      " |      `norm` is the user-chosen normalization to use. Accepted values are: 'l1' and 'l2'.\n",
      " |      \n",
      " |      If `num_best` is left unspecified, similarity queries will return a full\n",
      " |      vector with one float for every document in the index:\n",
      " |      \n",
      " |      >>> index = Similarity('/path/to/index', corpus, num_features=400) # if corpus has 7 documents...\n",
      " |      >>> index[query] # ... then result will have 7 floats\n",
      " |      [0.0, 0.0, 0.2, 0.13, 0.8, 0.0, 0.1]\n",
      " |      \n",
      " |      If `num_best` is set, queries return only the `num_best` most similar documents,\n",
      " |      always leaving out documents for which the similarity is 0.\n",
      " |      If the input vector itself only has features with zero values (=the sparse\n",
      " |      representation is empty), the returned list will always be empty.\n",
      " |      \n",
      " |      >>> index.num_best = 3\n",
      " |      >>> index[query] # return at most \"num_best\" of `(index_of_document, similarity)` tuples\n",
      " |      [(4, 0.8), (2, 0.13), (3, 0.13)]\n",
      " |      \n",
      " |      You can also override `num_best` dynamically, simply by setting e.g.\n",
      " |      `self.num_best = 10` before doing a query.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      For each index document, compute cosine similarity against all other\n",
      " |      documents in the index and yield the result.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_documents(self, corpus)\n",
      " |      Extend the index with new documents.\n",
      " |      \n",
      " |      Internally, documents are buffered and then spilled to disk when there's\n",
      " |      `self.shardsize` of them (or when a query is issued).\n",
      " |  \n",
      " |  check_moved(self)\n",
      " |      Update shard locations, in case the server directory has moved on filesystem.\n",
      " |  \n",
      " |  close_shard(self)\n",
      " |      Force the latest shard to close (be converted to a matrix and stored\n",
      " |      to disk). Do nothing if no new documents added since last call.\n",
      " |      \n",
      " |      **NOTE**: the shard is closed even if it is not full yet (its size is smaller\n",
      " |      than `self.shardsize`). If documents are added later via `add_documents()`,\n",
      " |      this incomplete shard will be loaded again and completed.\n",
      " |  \n",
      " |  destroy(self)\n",
      " |      Delete all files under self.output_prefix. Object is not usable after calling\n",
      " |      this method anymore. Use with care!\n",
      " |  \n",
      " |  iter_chunks(self, chunksize=None)\n",
      " |      Iteratively yield the index as chunks of documents, each of size <= chunksize.\n",
      " |      \n",
      " |      The chunk is returned in its raw form (matrix or sparse matrix slice).\n",
      " |      The size of the chunk may be smaller than requested; it is up to the caller\n",
      " |      to check the result for real length, using `chunk.shape[0]`.\n",
      " |  \n",
      " |  query_shards(self, query)\n",
      " |      Return the result of applying shard[query] for each shard in self.shards,\n",
      " |      as a sequence.\n",
      " |      \n",
      " |      If PARALLEL_SHARDS is set, the shards are queried in parallel, using\n",
      " |      the multiprocessing module.\n",
      " |  \n",
      " |  reopen_shard(self)\n",
      " |  \n",
      " |  save(self, fname=None, *args, **kwargs)\n",
      " |      Save the object via pickling (also see load) under filename specified in\n",
      " |      the constructor.\n",
      " |      \n",
      " |      Calls `close_shard` internally to spill any unfinished shards to disk first.\n",
      " |  \n",
      " |  shardid2filename(self, shardid)\n",
      " |  \n",
      " |  similarity_by_id(self, docpos)\n",
      " |      Return similarity of the given document only. `docpos` is the position\n",
      " |      of the query document within index.\n",
      " |  \n",
      " |  vector_by_id(self, docpos)\n",
      " |      Return indexed vector corresponding to the document at position `docpos`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.interfaces.SimilarityABC:\n",
      " |  \n",
      " |  get_similarities(self, doc)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.similarities.Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpc_id2name_path = './data/gpc_id2name.tsv'\n",
    "\n",
    "model2 = similar_model(gpc_id2name_path)\n",
    "model2.train_tfidf(pred_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potteri', 'glass', 'potteri', 'china', 'china', 'dinnerwar', 'meakin', 'j.', 'g', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_res= model2.predict(new_x_test[0])\n",
    "top5_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load ebat alive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16996\n"
     ]
    }
   ],
   "source": [
    "#读取 ebay category数据,获得 us 数据\n",
    "df = pd.read_csv('./data/ares-presto_run_4_stmt_1_0.csv',sep=',')\n",
    "# df=df.filter('site_id==0')、\n",
    "df = df[df[\"site_id\"] == 0]\n",
    "df=df.query(\"leaf_categ_id==move_to\")\n",
    "print(len(df))\n",
    "test = df[['leaf_categ_name']].values\n",
    "test =  test.reshape(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i = 10\n",
    "# test[i:i+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 232\n",
    "gpc_id2name_path = './data/gpc_id2name.tsv'\n",
    "\n",
    "pred_label = model.predict(test[index:index+1])\n",
    "model2 = similar_model(gpc_id2name_path)\n",
    "model2.train_tfidf(pred_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebay category    : Jewelry & Watches:Men's Jewelry:Bolo Ties\n",
      "\n",
      "google category_0: Apparel & Accessories > Jewelry > Watches\n",
      "google category_1: Apparel & Accessories > Jewelry > Jewelry Sets\n",
      "google category_2: Apparel & Accessories > Jewelry > Body Jewelry\n",
      "google category_3: Apparel & Accessories > Jewelry > Watch Accessories > Watch Bands\n",
      "google category_4: Apparel & Accessories > Jewelry > Watch Accessories > Watch Winders\n"
     ]
    }
   ],
   "source": [
    "top5_res= model2.predict(test[index])\n",
    "print(\"ebay category    :\",test[index])\n",
    "#print(class_map[pred_label[0]])\n",
    "print()\n",
    "\n",
    "for i in range(len(top5_res)):\n",
    "    print(\"google category_{}:\".format(i),top5_res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real google category:Apparel & Accessories > Jewelry > Watch Accessories\n"
     ]
    }
   ],
   "source": [
    "df_finn = pd.read_csv('./data/ebay2gg_table')\n",
    "real = df_finn.query('leaf_categ_name==\"{}\"'.format(test[index])).values[0][1]\n",
    "print(\"real google category:\"+real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "count=0\n",
    "new=0\n",
    "for index in trange(100):\n",
    "    pred_label = model.predict(test[index:index+1])\n",
    "    model2 = similar_model(gpc_id2name_path)\n",
    "    model2.train_tfidf(pred_label[0])\n",
    "    top5_res= model2.predict(test[index])\n",
    "    y_pred = top5_res[0]\n",
    "    if len(df_finn.query('leaf_categ_name==\"{}\"'.format(test[index])))==0:\n",
    "        new+=1\n",
    "        continue\n",
    "    y_real = df_finn.query('leaf_categ_name==\"{}\"'.format(test[index])).values[0][1]\n",
    "    \n",
    "    if y_real == y_pred:\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26804123711340205\n"
     ]
    }
   ],
   "source": [
    "print(count/(100-new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Similarity in module gensim.similarities.docsim:\n",
      "\n",
      "class Similarity(gensim.interfaces.SimilarityABC)\n",
      " |  Compute cosine similarity of a dynamic query against a static corpus of documents\n",
      " |  (\"the index\").\n",
      " |  \n",
      " |  Scalability is achieved by sharding the index into smaller pieces, each of which\n",
      " |  fits into core memory (see the `(Sparse)MatrixSimilarity` classes in this module).\n",
      " |  The shards themselves are simply stored as files to disk and mmap'ed back as needed.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Similarity\n",
      " |      gensim.interfaces.SimilarityABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, query)\n",
      " |      Get similarities of document `query` to all documents in the corpus.\n",
      " |      \n",
      " |      **or**\n",
      " |      \n",
      " |      If `query` is a corpus (iterable of documents), return a matrix of similarities\n",
      " |      of all query documents vs. all corpus document. This batch query is more\n",
      " |      efficient than computing the similarities one document after another.\n",
      " |  \n",
      " |  __init__(self, output_prefix, corpus, num_features, num_best=None, chunksize=256, shardsize=32768, norm='l2')\n",
      " |      Construct the index from `corpus`. The index can be later extended by calling\n",
      " |      the `add_documents` method. **Note**: documents are split (internally, transparently)\n",
      " |      into shards of `shardsize` documents each, converted to a matrix, for faster BLAS calls.\n",
      " |      Each shard is stored to disk under `output_prefix.shard_number` (=you need write\n",
      " |      access to that location). If you don't specify an output prefix, a random\n",
      " |      filename in temp will be used.\n",
      " |      \n",
      " |      `shardsize` should be chosen so that a `shardsize x chunksize` matrix of floats\n",
      " |      fits comfortably into main memory.\n",
      " |      \n",
      " |      `num_features` is the number of features in the `corpus` (e.g. size of the\n",
      " |      dictionary, or the number of latent topics for latent semantic models).\n",
      " |      \n",
      " |      `norm` is the user-chosen normalization to use. Accepted values are: 'l1' and 'l2'.\n",
      " |      \n",
      " |      If `num_best` is left unspecified, similarity queries will return a full\n",
      " |      vector with one float for every document in the index:\n",
      " |      \n",
      " |      >>> index = Similarity('/path/to/index', corpus, num_features=400) # if corpus has 7 documents...\n",
      " |      >>> index[query] # ... then result will have 7 floats\n",
      " |      [0.0, 0.0, 0.2, 0.13, 0.8, 0.0, 0.1]\n",
      " |      \n",
      " |      If `num_best` is set, queries return only the `num_best` most similar documents,\n",
      " |      always leaving out documents for which the similarity is 0.\n",
      " |      If the input vector itself only has features with zero values (=the sparse\n",
      " |      representation is empty), the returned list will always be empty.\n",
      " |      \n",
      " |      >>> index.num_best = 3\n",
      " |      >>> index[query] # return at most \"num_best\" of `(index_of_document, similarity)` tuples\n",
      " |      [(4, 0.8), (2, 0.13), (3, 0.13)]\n",
      " |      \n",
      " |      You can also override `num_best` dynamically, simply by setting e.g.\n",
      " |      `self.num_best = 10` before doing a query.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      For each index document, compute cosine similarity against all other\n",
      " |      documents in the index and yield the result.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_documents(self, corpus)\n",
      " |      Extend the index with new documents.\n",
      " |      \n",
      " |      Internally, documents are buffered and then spilled to disk when there's\n",
      " |      `self.shardsize` of them (or when a query is issued).\n",
      " |  \n",
      " |  check_moved(self)\n",
      " |      Update shard locations, in case the server directory has moved on filesystem.\n",
      " |  \n",
      " |  close_shard(self)\n",
      " |      Force the latest shard to close (be converted to a matrix and stored\n",
      " |      to disk). Do nothing if no new documents added since last call.\n",
      " |      \n",
      " |      **NOTE**: the shard is closed even if it is not full yet (its size is smaller\n",
      " |      than `self.shardsize`). If documents are added later via `add_documents()`,\n",
      " |      this incomplete shard will be loaded again and completed.\n",
      " |  \n",
      " |  destroy(self)\n",
      " |      Delete all files under self.output_prefix. Object is not usable after calling\n",
      " |      this method anymore. Use with care!\n",
      " |  \n",
      " |  iter_chunks(self, chunksize=None)\n",
      " |      Iteratively yield the index as chunks of documents, each of size <= chunksize.\n",
      " |      \n",
      " |      The chunk is returned in its raw form (matrix or sparse matrix slice).\n",
      " |      The size of the chunk may be smaller than requested; it is up to the caller\n",
      " |      to check the result for real length, using `chunk.shape[0]`.\n",
      " |  \n",
      " |  query_shards(self, query)\n",
      " |      Return the result of applying shard[query] for each shard in self.shards,\n",
      " |      as a sequence.\n",
      " |      \n",
      " |      If PARALLEL_SHARDS is set, the shards are queried in parallel, using\n",
      " |      the multiprocessing module.\n",
      " |  \n",
      " |  reopen_shard(self)\n",
      " |  \n",
      " |  save(self, fname=None, *args, **kwargs)\n",
      " |      Save the object via pickling (also see load) under filename specified in\n",
      " |      the constructor.\n",
      " |      \n",
      " |      Calls `close_shard` internally to spill any unfinished shards to disk first.\n",
      " |  \n",
      " |  shardid2filename(self, shardid)\n",
      " |  \n",
      " |  similarity_by_id(self, docpos)\n",
      " |      Return similarity of the given document only. `docpos` is the position\n",
      " |      of the query document within index.\n",
      " |  \n",
      " |  vector_by_id(self, docpos)\n",
      " |      Return indexed vector corresponding to the document at position `docpos`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.interfaces.SimilarityABC:\n",
      " |  \n",
      " |  get_similarities(self, doc)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.similarities.Similarity)\n",
    "self.sims = gensim.similarities.Similarity('./',self.tf_idf[corpus],num_features=len(self.dictionary),num_best=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
