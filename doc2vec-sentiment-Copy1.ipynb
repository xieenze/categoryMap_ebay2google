{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument \n",
    "from gensim.models import Doc2Vec\n",
    "import numpy\n",
    "import random\n",
    "import os\n",
    "import nltk.stem as stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(TaggedDocument (utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "rootPath='data/dataset/'\n",
    "sources = {\n",
    "            rootPath + 'Animals & Pet Supplies_new.txt':'TRAIN_1', \n",
    "            rootPath + 'Apparel & Accessories_new.txt':'TRAIN_2',\n",
    "            rootPath + 'Arts & Entertainment_new.txt':'TRAIN_3',\n",
    "            rootPath + 'Baby & Toddler_new.txt':'TRAIN_4',\n",
    "            rootPath + 'Business & Industrial_new.txt':'TRAIN_5',\n",
    "            rootPath + 'Cameras & Optics_new.txt':'TRAIN_6',\n",
    "            rootPath + 'Electronics_new.txt':'TRAIN_7',\n",
    "            rootPath + 'Food, Beverages & Tobacco_new.txt':'TRAIN_8',\n",
    "            rootPath + 'Furniture_new.txt':'TRAIN_9',\n",
    "            rootPath + 'Hardware_new.txt':'TRAIN_10',\n",
    "            rootPath + 'Health & Beauty_new.txt':'TRAIN_11',\n",
    "            rootPath + 'Home & Garden_new.txt':'TRAIN_12',\n",
    "            rootPath + 'Luggage & Bags_new.txt':'TRAIN_13',\n",
    "            rootPath + 'Mature_new.txt':'TRAIN_14',\n",
    "            rootPath + 'Media_new.txt':'TRAIN_15',\n",
    "            rootPath + 'Office Supplies_new.txt':'TRAIN_16',\n",
    "            rootPath + 'Religious & Ceremonial_new.txt':'TRAIN_17',\n",
    "            rootPath + 'Software_new.txt':'TRAIN_18',\n",
    "            rootPath + 'Sporting Goods_new.txt':'TRAIN_19',\n",
    "            rootPath + 'Toys & Games_new.txt':'TRAIN_20',\n",
    "            rootPath + 'Vehicles & Parts_new.txt':'TRAIN_21'\n",
    "          }\n",
    "#sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size=200\n",
    "model = Doc2Vec(min_count=1, window=10, size=feature_size, sample=1e-4, negative=5, workers=8)\n",
    "# model = Doc2Vec(min_count=1, window=10, size=feature_size, sample=1e-4, hs=1, workers=8)\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7394961"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs=model.iter+30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#总样本量\n",
    "model.corpus_count\n",
    "model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01168581, -0.4367393 ,  0.16919021,  0.22594388, -0.30299303,\n",
       "        0.11121316,  0.21889901,  0.13416478, -0.37329695,  0.31481901,\n",
       "       -0.34598714,  0.12764886,  0.12630624, -0.15212995, -0.09703127,\n",
       "        0.49712393,  0.12650813,  0.06289864, -0.07523122,  0.42763376,\n",
       "       -0.15300679, -0.38832864, -0.32271993, -0.52706462, -0.20038989,\n",
       "        0.20220351, -0.21860264,  0.32495293, -0.03611407,  0.09168439,\n",
       "        0.49789193, -0.53510505, -0.27024904, -0.34584367,  0.4115319 ,\n",
       "        0.38744739,  0.21230787, -0.29203212, -0.36506402,  0.02270904,\n",
       "       -0.46085957, -0.38591906,  0.42643157, -0.12298442, -0.07797596,\n",
       "        0.37000018,  0.06252047,  0.02975133,  0.00896106,  0.36006603,\n",
       "        0.13792478,  0.45265916, -0.29365671,  0.06642924,  0.0770029 ,\n",
       "       -0.00942502,  0.28842881,  0.11663827, -0.29112527, -0.2240781 ,\n",
       "        0.59489971, -0.0053976 , -0.01053664, -0.27593428,  0.27380082,\n",
       "       -0.2012037 ,  0.22448991, -0.2355428 ,  0.3568624 , -0.02316289,\n",
       "        0.27361661,  0.39396209,  0.01446457, -0.12040218,  0.02214159,\n",
       "       -0.31478599, -0.19572906, -0.25968724,  0.10852515,  0.32494941,\n",
       "       -0.28792882, -0.11458799, -0.12945703, -0.68535763, -0.13722359,\n",
       "        0.3334299 , -0.13209595, -0.17574124,  0.29280162,  0.13240017,\n",
       "       -0.58947539,  0.22795206,  0.58004349,  0.10696312, -0.35701081,\n",
       "        0.30541748,  0.15177405, -0.24374746, -0.14705192,  0.263217  ,\n",
       "       -0.23587012,  0.04343927,  0.18213917, -0.18609922,  0.05496889,\n",
       "       -0.14542316, -0.40333191,  0.03401627,  0.17064495,  0.11926781,\n",
       "       -0.09481236,  0.00502388,  0.21623015, -0.27017093,  0.17754708,\n",
       "       -0.22775686, -0.24501732,  0.20389508,  0.06706291, -0.06644386,\n",
       "       -0.15473691,  0.25435567, -0.31493387, -0.31963459,  0.52193344,\n",
       "        0.02673898,  0.03278834,  0.30741093, -0.00462072, -0.03078134,\n",
       "       -0.25447437,  0.29315883,  0.15199302,  0.47652715, -0.23619524,\n",
       "       -0.16623268, -0.46739537,  0.1528772 , -0.18532413, -0.40083179,\n",
       "       -0.03186727, -0.66828012, -0.32109228, -0.16296725,  0.24113035,\n",
       "        0.02133524,  0.06341311, -0.4921777 ,  0.36656839, -0.42262843,\n",
       "        0.04210966, -0.2747786 , -0.17396796,  0.10483088, -0.01619919,\n",
       "       -0.18072066, -0.3470425 ,  0.10442314, -0.26734114,  0.17574911,\n",
       "        0.02035781, -0.63278419,  0.03420114,  0.65421098,  0.43368262,\n",
       "       -0.25711286,  0.14629605, -0.17698631,  0.17917247, -0.01337991,\n",
       "       -0.16865268,  0.24285571,  0.36028391, -0.28498465,  0.16653407,\n",
       "       -0.58135015, -0.32566908, -0.19471261,  0.14821425,  0.32294318,\n",
       "        0.11129496,  0.07243   ,  0.37823334, -0.21084364,  0.47108129,\n",
       "       -0.04547827, -0.23174426,  0.37922952, -0.24371164, -0.23022355,\n",
       "       -0.30176792, -0.04086822,  0.1703523 , -0.11037435, -0.25127584,\n",
       "        0.06698433, -0.07215395,  0.00290356, -0.01351408, -0.23181008], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN_1的第一条记录 的 特征\n",
    "model.docvecs['TRAIN_20_503']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "\n",
    "### Training Vectors\n",
    "\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative).\n",
    "\n",
    "Hence, we create a `numpy` array (since the classifier we use only takes numpy arrays. There are two parallel arrays, one containing the vectors (`train_arrays`) and the other containing the labels (`train_labels`).\n",
    "\n",
    "We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((37666, feature_size))\n",
    "train_labels = numpy.zeros(37666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sources = {v:k for k,v in sources.items()} \n",
    "#new_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 429\n",
      "429 6241\n",
      "6241 16014\n",
      "16014 16155\n",
      "16155 19701\n",
      "19701 20313\n",
      "20313 24755\n",
      "24755 24935\n",
      "24935 25023\n",
      "25023 25343\n",
      "25343 27018\n",
      "27018 30303\n",
      "30303 30354\n",
      "30354 30388\n",
      "30388 30852\n",
      "30852 31175\n",
      "31175 31206\n",
      "31206 31556\n",
      "31556 35179\n",
      "35179 37472\n",
      "37472 37666\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "label_index = 0\n",
    "for i in range(1,22,1):\n",
    "    prefix_train_pos = 'TRAIN_' + str(i) + '_'\n",
    "    count = len(open(new_sources['TRAIN_'+str(i)]).readlines())\n",
    "    print(index,index+count)\n",
    "    for j in range(index,index+count,1):\n",
    "        #print(prefix_train_pos+str(j))\n",
    "        train_arrays[j] = model.docvecs[prefix_train_pos+str(j-index)]\n",
    "        train_labels[j] = label_index\n",
    "    index = index+count\n",
    "    label_index =label_index+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label to one-hot\n",
    "# from keras.utils import np_utils\n",
    "# train_labels = np_utils.to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 6240\n",
    "# print(train_arrays[index])\n",
    "# print(train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X,Y = shuffle(train_arrays,train_labels,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train,y_test = train_test_split(X,Y,test_size=0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now we train a logistic regression classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ensemble\n",
    "gbdt = ensemble.GradientBoostingClassifier(n_estimators=20,subsample=0.7,verbose =1)\n",
    "#gbdt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=8,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.ensemble as ensemble\n",
    "\n",
    "rf_classifer = sklearn.ensemble.RandomForestClassifier(n_estimators=100, \n",
    "                                                       max_depth=None,\n",
    "                                                       min_samples_split=2,\n",
    "                                                       min_samples_leaf=1,\n",
    "                                                       min_weight_fraction_leaf=0.0,\n",
    "                                                       max_leaf_nodes=None, \n",
    "                                                       min_impurity_decrease=0.0, \n",
    "                                                       min_impurity_split=None,\n",
    "                                                       bootstrap=True,\n",
    "                                                       oob_score=False,\n",
    "                                                       n_jobs=8,\n",
    "                                                       random_state=None, \n",
    "                                                       verbose=1,\n",
    "                                                       warm_start=False,\n",
    "                                                       class_weight=None)\n",
    "rf_classifer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38791547201868959"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifer.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/enxie/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_2 to have shape (None, 21) but got array with shape (28249, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-97d3eba49289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1419\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1420\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_2 to have shape (None, 21) but got array with shape (28249, 1)"
     ]
    }
   ],
   "source": [
    "#单层 神经网络  73%\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=150, input_dim=feature_size))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(units=21))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True),metrics=[ 'acc'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128,verbose=1,validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/dataset/Mature.txt','r')\n",
    "#f_new = open('data/dataset/Mature.txt','w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in f:\n",
    "#     l = line.split(\" \")\n",
    "#     j_new = str(l).replace(\",\" ,\"\").replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"\\\\n\",\"\\n\")\n",
    "#     print(j_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt = os.listdir('./data/dataset')[1:]\n",
    "for i in listt:\n",
    "    path = os.path.join(\"./data/dataset\",i)\n",
    "    f = open(path,'r')\n",
    "    f_new = open(path.replace(\".txt\",\"\")+\"_new.txt\",'w')\n",
    "    for j in f:\n",
    "        j=j.lower().replace(\",\",\" \").replace(\"'\",\" \").replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "        #提取词干\n",
    "#         js = j.split(\" \")\n",
    "#         js_new = [s.stem(i) for i in js]\n",
    "#         j_new = str(js_new).replace(\",\" ,\"\").replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"\\\\n\",\"\\n\")\n",
    "        #j_new=j_new.lower().replace(\",\",\" \").replace(\"'\",\" \").replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "        f_new.write(j)\n",
    "    f.close()\n",
    "    f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
