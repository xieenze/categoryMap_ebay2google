{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "from imblearn.combine import SMOTEENN \n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import ADASYN \n",
    "from imblearn.ensemble import BalanceCascade\n",
    "import imblearn\n",
    "\n",
    "import nltk.stem as stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_finn = pd.read_csv('../data/ebay2gg_table')\n",
    "#df_finn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_gg2id = pd.read_csv('../data/gpc_id2name.tsv',sep=\"\\t\")\n",
    "#df_gg2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5427/5427 [00:26<00:00, 203.10it/s]\n"
     ]
    }
   ],
   "source": [
    "gg_cate = list(df_gg2id['GPC_NAME'])\n",
    "\n",
    "sql = 'GPC_NAME==\"need_replace\"'\n",
    "'''\n",
    "X是ebay cate，如 Jewelry & Watches:Vintage & Antique Jewelry:Fine:Designer, Signed:Rings\n",
    "y是对应google cate的label(类别),在0-1997之间的数\n",
    "d是一个dict，key为label值,value为对应的google cate的名字 如  {0: 'Animals & Pet Supplies'}\n",
    "'''\n",
    "X=[]\n",
    "y=[]\n",
    "d=[]\n",
    "X_1 = []\n",
    "y_1 = []\n",
    "label=0\n",
    "#label_1 = 0\n",
    "for i in trange(len(gg_cate)):\n",
    "    new_sql = sql.replace(\"need_replace\",gg_cate[i])\n",
    "    res_list = list(df_finn.query(new_sql)['leaf_categ_name'])\n",
    "    #如果样本数量大于等于3\n",
    "    #小于3的类别不参与预测，只参与训练\n",
    "    k=3\n",
    "    if len(res_list)>=k:\n",
    "        d.append([label,gg_cate[i]])\n",
    "        for j in res_list:\n",
    "            X.append(j)\n",
    "            y.append(label)\n",
    "        label+=1\n",
    "    elif len(res_list)>0 and len(res_list)<k:\n",
    "        d.append([label,gg_cate[i]])\n",
    "        for j in res_list:\n",
    "            X_1.append(j)\n",
    "            y_1.append(label)\n",
    "        label+=1\n",
    "d=dict(d)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''打乱数据，分割训练，验证集，可以设置按每一类比例随机抽（stratify =y）'''\n",
    "#X,y=shuffle(X,y)\n",
    "X_train , X_test , y_train  , y_test = train_test_split(X,y,test_size=0.3,stratify =y,shuffle=True) \n",
    "#X_train , X_test , y_train  , y_test = train_test_split(X,y,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25531 10943\n",
      "26723 26723\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))\n",
    "X_train = np.concatenate((X_train,X_1),axis=0)\n",
    "y_train = np.concatenate((y_train,y_1),axis=0)\n",
    "print(len(X_train),len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "构建pipeline，包括了tfidf抽取特征和 rf分类\n",
    "'''\n",
    "# pipeline = make_pipeline(TfidfVectorizer(),GradientBoostingClassifier(n_estimators=50,verbose=1))\n",
    "from sklearn import random_projection\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import manifold\n",
    "\n",
    "#https://www.jianshu.com/p/d59cf1618dbd\n",
    "pipeline = make_pipeline(TfidfVectorizer(),\n",
    "                         random_projection.SparseRandomProjection(),\n",
    "                         RandomForestClassifier(n_estimators=20,n_jobs=-1,oob_score=True))\n",
    "\n",
    "# pipeline = make_pipeline_imb(TfidfVectorizer(),\n",
    "#                          #ADASYN(n_neighbors=2,n_jobs=4),\n",
    "#                          #RandomOverSampler(),\n",
    "#                          SMOTE(random_state=0,n_jobs=4,k_neighbors=2),\n",
    "#                              #SMOTEENN(random_state=42),\n",
    "#                             # BalanceCascade(random_state=42)\n",
    "#                          RandomForestClassifier(n_estimators=20,n_jobs=4))\n",
    "\n",
    "'''训练模型'''\n",
    "pipeline.fit(X_train, y_train)\n",
    "'''交叉验证'''\n",
    "# scores = cross_val_score(pipeline, X_train, y_train,n_jobs=2,cv=5,verbose=1)\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''模型的准确率'''\n",
    "pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/rf_2.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, '../model/rf_2.pkl',compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9068399299996277"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = joblib.load('../model/rf.pkl')\n",
    "model.score(X_train,y_train)\n",
    "\n",
    "\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and use averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool (default=False)\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\",\n",
      " |      \"balanced_subsample\" or None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>>\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      " |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      " |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |              min_samples_leaf=1, min_samples_split=2,\n",
      " |              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      " |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(clf.feature_importances_)\n",
      " |  [ 0.17287856  0.80608704  0.01884792  0.00218648]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.get_params()\n",
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.18      0.29        11\n",
      "          1       0.00      0.00      0.00         2\n",
      "          2       0.00      0.00      0.00         1\n",
      "          3       0.00      0.00      0.00         1\n",
      "          4       0.50      0.50      0.50         2\n",
      "          5       0.60      0.60      0.60         5\n",
      "          6       0.59      0.91      0.71        11\n",
      "          7       1.00      1.00      1.00         1\n",
      "          8       0.97      0.97      0.97        70\n",
      "          9       0.80      1.00      0.89         8\n",
      "         10       1.00      0.50      0.67         2\n",
      "         11       0.00      0.00      0.00         1\n",
      "         12       0.85      0.93      0.89        30\n",
      "         13       0.00      0.00      0.00         1\n",
      "         14       0.43      1.00      0.60         3\n",
      "         15       0.67      0.67      0.67        18\n",
      "         16       0.00      0.00      0.00         1\n",
      "         17       1.00      0.25      0.40         4\n",
      "         18       0.00      0.00      0.00         1\n",
      "         19       0.00      0.00      0.00         1\n",
      "         20       0.00      0.00      0.00         1\n",
      "         21       1.00      0.67      0.80         3\n",
      "         22       1.00      1.00      1.00         1\n",
      "         23       1.00      1.00      1.00         1\n",
      "         24       0.00      0.00      0.00         2\n",
      "         25       0.78      0.78      0.78         9\n",
      "         26       0.00      0.00      0.00         1\n",
      "         27       1.00      1.00      1.00         4\n",
      "         28       0.00      0.00      0.00         2\n",
      "         29       0.50      0.33      0.40         3\n",
      "         30       0.00      0.00      0.00         1\n",
      "         31       1.00      0.50      0.67         2\n",
      "         32       0.74      0.96      0.84       236\n",
      "         33       0.75      0.67      0.71        18\n",
      "         34       1.00      0.33      0.50         3\n",
      "         35       0.80      0.90      0.84       139\n",
      "         36       0.72      0.86      0.78        21\n",
      "         37       0.60      0.75      0.67         4\n",
      "         38       0.95      0.83      0.89        24\n",
      "         39       1.00      0.82      0.90        45\n",
      "         40       1.00      1.00      1.00        16\n",
      "         41       0.85      0.61      0.71        18\n",
      "         42       0.00      0.00      0.00         5\n",
      "         43       1.00      0.71      0.83        17\n",
      "         44       0.92      0.92      0.92        13\n",
      "         45       1.00      0.93      0.97        15\n",
      "         46       0.75      0.75      0.75         4\n",
      "         47       0.88      0.72      0.79        60\n",
      "         48       1.00      1.00      1.00        12\n",
      "         49       0.67      0.40      0.50         5\n",
      "         50       0.00      0.00      0.00         1\n",
      "         51       0.86      0.76      0.81        41\n",
      "         52       0.89      0.87      0.88        54\n",
      "         53       1.00      0.86      0.92         7\n",
      "         54       0.92      0.86      0.89        14\n",
      "         55       0.82      0.75      0.78        12\n",
      "         56       1.00      0.81      0.90        27\n",
      "         57       0.95      0.96      0.95        98\n",
      "         58       0.87      0.79      0.83       101\n",
      "         59       0.85      0.92      0.88        12\n",
      "         60       0.83      1.00      0.91        24\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.80      0.86      0.83        14\n",
      "         63       1.00      0.43      0.60         7\n",
      "         64       0.88      0.50      0.64        14\n",
      "         65       0.67      1.00      0.80         2\n",
      "         66       0.68      0.65      0.67        20\n",
      "         67       0.50      0.25      0.33         8\n",
      "         68       0.00      0.00      0.00         1\n",
      "         69       1.00      0.67      0.80        15\n",
      "         70       0.71      0.62      0.67         8\n",
      "         71       0.83      0.83      0.83         6\n",
      "         72       0.77      0.85      0.81        27\n",
      "         73       0.36      0.31      0.33        16\n",
      "         74       0.00      0.00      0.00         1\n",
      "         75       1.00      0.33      0.50         3\n",
      "         76       0.00      0.00      0.00         1\n",
      "         77       1.00      1.00      1.00         9\n",
      "         78       0.50      0.50      0.50         2\n",
      "         79       0.68      0.84      0.75        63\n",
      "         80       0.91      0.91      0.91        54\n",
      "         81       0.75      0.50      0.60        12\n",
      "         82       0.72      0.72      0.72        18\n",
      "         83       0.00      0.00      0.00         1\n",
      "         84       0.67      1.00      0.80         4\n",
      "         85       0.93      0.91      0.92        56\n",
      "         86       1.00      0.67      0.80         3\n",
      "         87       0.00      0.00      0.00         2\n",
      "         88       0.50      0.50      0.50         2\n",
      "         89       0.00      0.00      0.00         1\n",
      "         90       0.80      0.83      0.82        24\n",
      "         91       0.00      0.00      0.00         2\n",
      "         92       0.00      0.00      0.00         3\n",
      "         93       1.00      0.55      0.71        11\n",
      "         94       0.75      0.86      0.80         7\n",
      "         95       0.00      0.00      0.00         1\n",
      "         96       0.00      0.00      0.00         1\n",
      "         97       0.00      0.00      0.00         2\n",
      "         98       0.86      0.86      0.86         7\n",
      "         99       0.87      0.79      0.83        33\n",
      "        100       0.00      0.00      0.00         5\n",
      "        101       1.00      0.33      0.50         6\n",
      "        102       0.71      0.83      0.77         6\n",
      "        103       1.00      0.88      0.93         8\n",
      "        104       1.00      1.00      1.00        40\n",
      "        105       0.00      0.00      0.00         3\n",
      "        106       0.00      0.00      0.00         1\n",
      "        107       0.89      0.94      0.92        18\n",
      "        108       0.67      0.50      0.57         8\n",
      "        109       1.00      0.50      0.67         4\n",
      "        110       0.00      0.00      0.00         1\n",
      "        111       0.50      1.00      0.67         1\n",
      "        112       0.86      0.67      0.75         9\n",
      "        113       1.00      1.00      1.00         4\n",
      "        114       1.00      1.00      1.00         1\n",
      "        115       0.79      0.63      0.70        35\n",
      "        116       0.57      0.67      0.62         6\n",
      "        117       0.33      0.40      0.36         5\n",
      "        118       0.97      1.00      0.98       172\n",
      "        119       1.00      0.94      0.97        67\n",
      "        120       0.99      0.99      0.99       207\n",
      "        121       0.99      0.99      0.99       172\n",
      "        122       0.96      0.96      0.96        45\n",
      "        123       1.00      0.93      0.96        41\n",
      "        124       0.94      0.96      0.95        92\n",
      "        125       0.95      0.95      0.95       395\n",
      "        126       1.00      0.94      0.97        17\n",
      "        127       0.00      0.00      0.00         1\n",
      "        128       0.67      0.67      0.67         9\n",
      "        129       0.00      0.00      0.00         1\n",
      "        130       0.63      0.73      0.68        71\n",
      "        131       0.88      0.73      0.80        30\n",
      "        132       1.00      0.33      0.50         3\n",
      "        133       0.43      0.67      0.52        27\n",
      "        134       0.64      0.69      0.67        13\n",
      "        135       0.00      0.00      0.00         1\n",
      "        136       0.91      0.71      0.80        28\n",
      "        137       0.86      1.00      0.92        36\n",
      "        138       0.85      0.96      0.90        24\n",
      "        139       0.71      0.50      0.59        10\n",
      "        140       0.83      0.96      0.89        50\n",
      "        141       0.36      0.30      0.33        33\n",
      "        142       1.00      0.67      0.80         3\n",
      "        143       0.90      0.92      0.91        39\n",
      "        144       0.79      0.44      0.56        50\n",
      "        145       1.00      1.00      1.00         3\n",
      "        146       0.00      0.00      0.00         1\n",
      "        147       1.00      0.75      0.86         4\n",
      "        148       0.00      0.00      0.00         1\n",
      "        149       0.67      0.38      0.48        16\n",
      "        150       0.62      0.31      0.42        16\n",
      "        151       1.00      0.86      0.92         7\n",
      "        152       0.65      1.00      0.79        32\n",
      "        153       0.00      0.00      0.00         1\n",
      "        154       0.00      0.00      0.00         1\n",
      "        155       0.47      0.95      0.63        19\n",
      "        156       0.33      0.25      0.29         4\n",
      "        157       0.00      0.00      0.00         1\n",
      "        158       1.00      1.00      1.00         5\n",
      "        159       0.83      1.00      0.91         5\n",
      "        160       0.00      0.00      0.00         3\n",
      "        161       0.89      0.80      0.84        40\n",
      "        162       1.00      0.43      0.60         7\n",
      "        163       1.00      0.75      0.86         4\n",
      "        164       0.00      0.00      0.00         2\n",
      "        165       0.00      0.00      0.00         1\n",
      "        166       0.87      0.93      0.90        14\n",
      "        167       1.00      1.00      1.00         8\n",
      "        168       0.50      0.50      0.50         2\n",
      "        169       0.00      0.00      0.00         1\n",
      "        170       1.00      0.50      0.67         2\n",
      "        171       0.40      0.40      0.40         5\n",
      "        172       0.00      0.00      0.00         1\n",
      "        173       1.00      1.00      1.00         3\n",
      "        174       1.00      1.00      1.00         1\n",
      "        175       0.62      1.00      0.77         5\n",
      "        176       0.00      0.00      0.00         4\n",
      "        177       0.00      0.00      0.00         1\n",
      "        178       0.00      0.00      0.00         1\n",
      "        179       0.00      0.00      0.00         2\n",
      "        180       0.95      0.88      0.91        24\n",
      "        181       0.67      0.80      0.73         5\n",
      "        182       0.91      0.84      0.87        25\n",
      "        183       0.75      0.60      0.67        10\n",
      "        184       0.81      0.95      0.88      1475\n",
      "        185       0.74      0.81      0.77        21\n",
      "        186       0.99      0.97      0.98       352\n",
      "        187       0.88      0.90      0.89       210\n",
      "        188       0.75      0.38      0.50         8\n",
      "        189       0.62      0.67      0.64        12\n",
      "        190       0.00      0.00      0.00         2\n",
      "        191       0.00      0.00      0.00         2\n",
      "        192       0.75      0.23      0.35        13\n",
      "        193       0.96      0.93      0.95       251\n",
      "        194       0.95      0.88      0.92       162\n",
      "        195       0.71      0.72      0.72       123\n",
      "        196       0.80      0.60      0.69        20\n",
      "        197       0.80      0.89      0.84         9\n",
      "        198       0.88      0.88      0.88        16\n",
      "        199       0.70      0.78      0.74         9\n",
      "        200       0.20      0.08      0.12        12\n",
      "        201       0.90      0.60      0.72        15\n",
      "        202       0.75      0.43      0.55         7\n",
      "        203       0.50      0.25      0.33         4\n",
      "        204       0.00      0.00      0.00         1\n",
      "        205       0.75      0.58      0.65        26\n",
      "        206       0.93      0.85      0.88        59\n",
      "        207       0.97      0.92      0.94       191\n",
      "        208       0.95      0.90      0.92        61\n",
      "        209       0.96      0.94      0.95       108\n",
      "        210       1.00      0.93      0.97        61\n",
      "        211       0.80      0.50      0.62         8\n",
      "        212       0.00      0.00      0.00         1\n",
      "        213       0.71      0.68      0.69       199\n",
      "        214       1.00      0.62      0.77         8\n",
      "        215       0.00      0.00      0.00         1\n",
      "        216       1.00      0.33      0.50         3\n",
      "        217       0.00      0.00      0.00         1\n",
      "        218       1.00      0.83      0.91         6\n",
      "        219       0.95      0.98      0.96        41\n",
      "        220       0.36      0.22      0.28        18\n",
      "        221       0.36      0.67      0.47         6\n",
      "        222       0.00      0.00      0.00         1\n",
      "        223       0.40      0.50      0.44         4\n",
      "        224       0.00      0.00      0.00         3\n",
      "        225       1.00      0.33      0.50         3\n",
      "        226       0.86      1.00      0.92        18\n",
      "        227       0.80      0.55      0.65        29\n",
      "        228       0.38      0.43      0.40         7\n",
      "        229       0.00      0.00      0.00         1\n",
      "        230       0.88      0.88      0.88        16\n",
      "        231       0.00      0.00      0.00         3\n",
      "        232       0.00      0.00      0.00         1\n",
      "        233       0.00      0.00      0.00         2\n",
      "        234       0.83      0.96      0.89        26\n",
      "        235       1.00      0.86      0.92         7\n",
      "        236       0.25      0.33      0.29         3\n",
      "        237       1.00      0.50      0.67         4\n",
      "        238       0.00      0.00      0.00         1\n",
      "        239       0.77      0.91      0.83        11\n",
      "        240       0.65      0.65      0.65        23\n",
      "        241       0.92      0.85      0.88        26\n",
      "        242       0.94      0.94      0.94        32\n",
      "        243       0.50      0.33      0.40         3\n",
      "        244       0.77      0.79      0.78        29\n",
      "        245       1.00      0.50      0.67         2\n",
      "        246       0.75      1.00      0.86         3\n",
      "        247       0.00      0.00      0.00         2\n",
      "        248       0.58      0.78      0.67         9\n",
      "        249       0.00      0.00      0.00         1\n",
      "        250       0.00      0.00      0.00         1\n",
      "        251       1.00      0.83      0.91         6\n",
      "        252       1.00      1.00      1.00         3\n",
      "        253       1.00      0.50      0.67         2\n",
      "        254       0.00      0.00      0.00         1\n",
      "        255       0.91      0.90      0.90        88\n",
      "        256       0.80      0.80      0.80         5\n",
      "        257       0.00      0.00      0.00         1\n",
      "        258       0.00      0.00      0.00         2\n",
      "        259       0.00      0.00      0.00         2\n",
      "        260       0.75      0.75      0.75         4\n",
      "        261       0.00      0.00      0.00         1\n",
      "        262       0.67      0.80      0.73       137\n",
      "        263       0.00      0.00      0.00         1\n",
      "        264       0.00      0.00      0.00         1\n",
      "        265       0.00      0.00      0.00         1\n",
      "        266       0.80      1.00      0.89         4\n",
      "        267       0.67      0.50      0.57         4\n",
      "        268       0.33      0.33      0.33         3\n",
      "        269       0.00      0.00      0.00         1\n",
      "        270       0.00      0.00      0.00         1\n",
      "        271       1.00      1.00      1.00         2\n",
      "        272       0.00      0.00      0.00         1\n",
      "        273       0.00      0.00      0.00         1\n",
      "        274       0.00      0.00      0.00         1\n",
      "        275       0.00      0.00      0.00         1\n",
      "        276       0.00      0.00      0.00         1\n",
      "        277       0.83      0.45      0.59        11\n",
      "        278       1.00      1.00      1.00         1\n",
      "        279       1.00      0.25      0.40         4\n",
      "        280       0.74      0.70      0.72        20\n",
      "        281       0.00      0.00      0.00         1\n",
      "        282       0.43      0.43      0.43         7\n",
      "        283       0.69      0.84      0.76        44\n",
      "        284       1.00      1.00      1.00         1\n",
      "        285       0.50      0.33      0.40         3\n",
      "        286       0.00      0.00      0.00         1\n",
      "        287       0.00      0.00      0.00         1\n",
      "        288       1.00      1.00      1.00         1\n",
      "        289       0.00      0.00      0.00         3\n",
      "        290       1.00      1.00      1.00         1\n",
      "        291       0.67      0.67      0.67         3\n",
      "        292       0.80      1.00      0.89         4\n",
      "        293       0.00      0.00      0.00         1\n",
      "        294       1.00      1.00      1.00         9\n",
      "        295       1.00      1.00      1.00         5\n",
      "        296       0.33      0.33      0.33         3\n",
      "        297       0.00      0.00      0.00         1\n",
      "        298       1.00      1.00      1.00         1\n",
      "        299       1.00      1.00      1.00         7\n",
      "        300       0.14      1.00      0.25         1\n",
      "        301       0.00      0.00      0.00         1\n",
      "        302       0.75      1.00      0.86         3\n",
      "        303       1.00      0.40      0.57         5\n",
      "        304       1.00      1.00      1.00         1\n",
      "        305       1.00      1.00      1.00         1\n",
      "        306       1.00      1.00      1.00         3\n",
      "        307       0.64      0.77      0.70        47\n",
      "        308       0.52      0.55      0.54       116\n",
      "        309       0.00      0.00      0.00         1\n",
      "        310       0.25      0.25      0.25         4\n",
      "        311       1.00      0.71      0.83        21\n",
      "        312       0.88      0.94      0.91       102\n",
      "        313       1.00      0.50      0.67         6\n",
      "        314       0.00      0.00      0.00         1\n",
      "        315       0.82      0.50      0.62        18\n",
      "        316       0.00      0.00      0.00         1\n",
      "        317       0.74      0.83      0.78        94\n",
      "        318       0.00      0.00      0.00         2\n",
      "        319       1.00      0.67      0.80         9\n",
      "        320       0.63      0.83      0.72       124\n",
      "        321       1.00      1.00      1.00         4\n",
      "        322       0.40      0.42      0.41        24\n",
      "        323       0.00      0.00      0.00         1\n",
      "        324       1.00      0.44      0.62         9\n",
      "        325       0.73      0.71      0.72       122\n",
      "        326       0.57      0.80      0.67         5\n",
      "        327       0.89      0.80      0.84        20\n",
      "        328       0.60      0.60      0.60         5\n",
      "        329       0.38      0.43      0.40         7\n",
      "        330       1.00      0.50      0.67         2\n",
      "        331       0.50      1.00      0.67         2\n",
      "        332       0.69      0.61      0.65        18\n",
      "        333       0.00      0.00      0.00         3\n",
      "        334       0.25      0.25      0.25         4\n",
      "        335       1.00      0.62      0.77         8\n",
      "        336       1.00      0.29      0.44         7\n",
      "        337       0.82      0.47      0.60        19\n",
      "        338       0.00      0.00      0.00         1\n",
      "        339       1.00      0.70      0.82        10\n",
      "        340       1.00      0.46      0.63        13\n",
      "        341       0.00      0.00      0.00         1\n",
      "        342       0.87      0.97      0.92       198\n",
      "        343       0.33      0.21      0.26        14\n",
      "        344       1.00      0.50      0.67         2\n",
      "        345       1.00      1.00      1.00         4\n",
      "        346       0.80      0.87      0.83       208\n",
      "        347       0.64      0.47      0.55        19\n",
      "        348       0.00      0.00      0.00         1\n",
      "        349       0.86      0.86      0.86         7\n",
      "        350       1.00      0.67      0.80         3\n",
      "        351       0.00      0.00      0.00         1\n",
      "        352       1.00      1.00      1.00         6\n",
      "        353       0.00      0.00      0.00         1\n",
      "        354       0.00      0.00      0.00         1\n",
      "        355       0.71      0.55      0.62        40\n",
      "        356       1.00      0.82      0.90        11\n",
      "        357       0.00      0.00      0.00         1\n",
      "        358       0.00      0.00      0.00         2\n",
      "        359       0.00      0.00      0.00         2\n",
      "        360       0.57      0.40      0.47        30\n",
      "        361       1.00      0.67      0.80         3\n",
      "        362       0.00      0.00      0.00         1\n",
      "        363       0.60      0.60      0.60         5\n",
      "        364       0.83      1.00      0.91         5\n",
      "        365       0.00      0.00      0.00         1\n",
      "        366       1.00      0.25      0.40         4\n",
      "        367       0.46      0.40      0.43        15\n",
      "        368       1.00      1.00      1.00         3\n",
      "        369       0.25      0.14      0.18         7\n",
      "        370       0.99      0.99      0.99        67\n",
      "        371       0.00      0.00      0.00         1\n",
      "        372       1.00      0.25      0.40         4\n",
      "        373       0.75      0.86      0.80         7\n",
      "        374       1.00      0.50      0.67         2\n",
      "        375       1.00      0.75      0.86         4\n",
      "        376       0.75      0.33      0.46         9\n",
      "        377       0.83      0.71      0.77         7\n",
      "        378       1.00      0.67      0.80         3\n",
      "        379       0.00      0.00      0.00         2\n",
      "        380       0.60      0.50      0.55        18\n",
      "        381       0.75      0.75      0.75         4\n",
      "        382       0.73      0.62      0.67        13\n",
      "        383       0.00      0.00      0.00         4\n",
      "        384       0.59      0.83      0.69        42\n",
      "        385       1.00      1.00      1.00         1\n",
      "        386       0.00      0.00      0.00         1\n",
      "        387       0.95      0.95      0.95        20\n",
      "        388       0.00      0.00      0.00         1\n",
      "        389       0.86      0.55      0.67        11\n",
      "        390       1.00      0.17      0.29         6\n",
      "        391       0.00      0.00      0.00         1\n",
      "        392       1.00      0.25      0.40         4\n",
      "        393       0.00      0.00      0.00         1\n",
      "        394       0.00      0.00      0.00         3\n",
      "        395       0.00      0.00      0.00         2\n",
      "        396       0.88      0.50      0.64        14\n",
      "        397       1.00      0.80      0.89         5\n",
      "        398       1.00      1.00      1.00         1\n",
      "        399       0.45      0.71      0.56        21\n",
      "        400       0.00      0.00      0.00         1\n",
      "        401       0.64      0.53      0.58        17\n",
      "        402       0.00      0.00      0.00         1\n",
      "        403       0.50      0.38      0.43        13\n",
      "        404       0.14      0.25      0.18         4\n",
      "        405       0.00      0.00      0.00         1\n",
      "        406       0.00      0.00      0.00         2\n",
      "        407       0.67      0.80      0.73         5\n",
      "        408       0.83      0.97      0.90       223\n",
      "        409       0.60      1.00      0.75         3\n",
      "        410       1.00      0.86      0.92        14\n",
      "        411       0.00      0.00      0.00         1\n",
      "        412       0.88      0.88      0.88         8\n",
      "        413       0.00      0.00      0.00         1\n",
      "        414       0.33      1.00      0.50         2\n",
      "        415       0.80      0.80      0.80         5\n",
      "        416       1.00      1.00      1.00         2\n",
      "        417       1.00      1.00      1.00         6\n",
      "        418       1.00      0.67      0.80         3\n",
      "        419       0.88      0.65      0.75        23\n",
      "        420       1.00      0.71      0.83         7\n",
      "        421       1.00      1.00      1.00         2\n",
      "        422       0.68      0.75      0.71        20\n",
      "        423       0.71      1.00      0.83         5\n",
      "        424       0.73      0.73      0.73        15\n",
      "        425       0.00      0.00      0.00         2\n",
      "        426       0.75      0.60      0.67         5\n",
      "        427       0.00      0.00      0.00         1\n",
      "        428       0.00      0.00      0.00         1\n",
      "        429       0.88      0.72      0.79        32\n",
      "        430       0.89      1.00      0.94         8\n",
      "        431       0.20      0.50      0.29         2\n",
      "        432       0.67      0.71      0.69        14\n",
      "        433       0.00      0.00      0.00         2\n",
      "        434       0.81      0.80      0.80        89\n",
      "        435       0.00      0.00      0.00         1\n",
      "        436       0.00      0.00      0.00         2\n",
      "        437       0.00      0.00      0.00         2\n",
      "        438       0.33      0.25      0.29         4\n",
      "        439       0.80      0.89      0.84         9\n",
      "        440       1.00      1.00      1.00         2\n",
      "        441       1.00      1.00      1.00         3\n",
      "        442       0.00      0.00      0.00         1\n",
      "        443       0.00      0.00      0.00         2\n",
      "        444       0.67      0.40      0.50         5\n",
      "        445       0.00      0.00      0.00         2\n",
      "        446       0.67      0.44      0.53         9\n",
      "        447       1.00      1.00      1.00         7\n",
      "        448       0.67      0.77      0.72        31\n",
      "        449       0.94      0.68      0.79        25\n",
      "        450       0.00      0.00      0.00         2\n",
      "        451       0.80      0.67      0.73         6\n",
      "        452       0.84      0.92      0.88       138\n",
      "        453       0.00      0.00      0.00         2\n",
      "        454       0.87      0.93      0.90        29\n",
      "        455       0.84      0.88      0.86       101\n",
      "        456       0.67      0.78      0.72        23\n",
      "        457       0.95      0.76      0.85        54\n",
      "        458       0.81      0.92      0.86        48\n",
      "        459       0.62      0.89      0.73         9\n",
      "        460       1.00      0.82      0.90        11\n",
      "        461       0.82      0.82      0.82        51\n",
      "        462       0.00      0.00      0.00         1\n",
      "        463       0.25      0.43      0.32         7\n",
      "        464       0.00      0.00      0.00         2\n",
      "        465       0.71      0.72      0.72        40\n",
      "        466       0.00      0.00      0.00         4\n",
      "        467       0.81      0.91      0.86       138\n",
      "        468       1.00      1.00      1.00         1\n",
      "        469       0.83      0.96      0.89        26\n",
      "        470       0.00      0.00      0.00         1\n",
      "        471       0.00      0.00      0.00         2\n",
      "        472       0.81      0.59      0.68        22\n",
      "        473       0.82      0.70      0.76        47\n",
      "        474       0.00      0.00      0.00         1\n",
      "        475       0.00      0.00      0.00         2\n",
      "        476       0.87      0.87      0.87        53\n",
      "        477       0.40      0.50      0.44         8\n",
      "        478       0.78      0.70      0.74        10\n",
      "        479       0.83      0.88      0.86        34\n",
      "        480       0.60      0.50      0.55        12\n",
      "        481       0.86      0.67      0.75         9\n",
      "        482       0.72      0.78      0.75        23\n",
      "        483       0.74      0.89      0.81        93\n",
      "        484       0.00      0.00      0.00         1\n",
      "        485       0.82      0.74      0.78        19\n",
      "        486       1.00      0.67      0.80         6\n",
      "        487       1.00      1.00      1.00        17\n",
      "        488       0.75      0.69      0.72        13\n",
      "        489       0.33      0.50      0.40         2\n",
      "        490       0.42      0.62      0.50         8\n",
      "        491       0.00      0.00      0.00         2\n",
      "        492       0.80      0.80      0.80        10\n",
      "        493       0.69      0.90      0.78        10\n",
      "        494       0.00      0.00      0.00         2\n",
      "        495       0.00      0.00      0.00         1\n",
      "        496       0.00      0.00      0.00         6\n",
      "        497       0.86      0.93      0.89        45\n",
      "        498       0.64      0.52      0.57        31\n",
      "        499       0.43      0.30      0.35        10\n",
      "        500       0.60      0.30      0.40        20\n",
      "        501       0.75      0.67      0.71         9\n",
      "        502       0.45      0.83      0.59         6\n",
      "        503       0.64      0.88      0.74         8\n",
      "        504       1.00      1.00      1.00         2\n",
      "        505       0.89      0.62      0.73        26\n",
      "        506       0.67      0.67      0.67         3\n",
      "        507       1.00      1.00      1.00         1\n",
      "        508       0.83      0.98      0.90        45\n",
      "        509       0.94      0.94      0.94        36\n",
      "        510       0.00      0.00      0.00         2\n",
      "        511       0.00      0.00      0.00         2\n",
      "        512       0.33      0.33      0.33         3\n",
      "        513       1.00      1.00      1.00         3\n",
      "        514       0.72      0.77      0.74        83\n",
      "        515       0.75      0.80      0.77        15\n",
      "        516       1.00      0.67      0.80         9\n",
      "        517       0.87      0.93      0.90        14\n",
      "        518       0.83      0.50      0.62        10\n",
      "        519       1.00      1.00      1.00         1\n",
      "        520       0.00      0.00      0.00         2\n",
      "        521       0.92      0.75      0.83        16\n",
      "        522       0.88      0.88      0.88        17\n",
      "        523       1.00      0.33      0.50         9\n",
      "        524       1.00      0.75      0.86         8\n",
      "        525       0.00      0.00      0.00         1\n",
      "        526       0.00      0.00      0.00         1\n",
      "        527       0.00      0.00      0.00         1\n",
      "        528       1.00      1.00      1.00         1\n",
      "        529       0.00      0.00      0.00         1\n",
      "        530       0.50      0.60      0.55         5\n",
      "        531       1.00      0.50      0.67         4\n",
      "        532       0.69      0.56      0.62        16\n",
      "        533       0.36      0.67      0.47         6\n",
      "        534       0.94      0.74      0.83        23\n",
      "        535       1.00      0.86      0.92        14\n",
      "        536       1.00      1.00      1.00         3\n",
      "        537       0.00      0.00      0.00         2\n",
      "        538       1.00      1.00      1.00         3\n",
      "        539       0.50      0.25      0.33         4\n",
      "        540       1.00      0.83      0.91         6\n",
      "        541       1.00      1.00      1.00         3\n",
      "        542       0.50      0.33      0.40         3\n",
      "        543       0.80      1.00      0.89        12\n",
      "        544       1.00      1.00      1.00         6\n",
      "        545       0.00      0.00      0.00         1\n",
      "        546       0.83      0.77      0.80        44\n",
      "        547       0.83      1.00      0.91         5\n",
      "        548       1.00      0.88      0.93         8\n",
      "        549       0.33      0.29      0.31         7\n",
      "        550       0.00      0.00      0.00         1\n",
      "        551       0.00      0.00      0.00         1\n",
      "        552       1.00      0.25      0.40         4\n",
      "        553       0.65      0.58      0.61        60\n",
      "        554       0.00      0.00      0.00         2\n",
      "        555       0.71      0.87      0.79       109\n",
      "        556       0.00      0.00      0.00         1\n",
      "        557       1.00      1.00      1.00         3\n",
      "        558       0.89      0.73      0.80        11\n",
      "        559       0.79      0.88      0.83        17\n",
      "        560       0.00      0.00      0.00         1\n",
      "        561       0.29      0.33      0.31         6\n",
      "        562       0.25      0.25      0.25         8\n",
      "        563       0.00      0.00      0.00         1\n",
      "        564       0.83      0.62      0.71         8\n",
      "        565       1.00      1.00      1.00         1\n",
      "        566       1.00      0.67      0.80         6\n",
      "        567       0.25      0.33      0.29         3\n",
      "        568       0.80      0.89      0.84         9\n",
      "        569       0.00      0.00      0.00         1\n",
      "        570       0.50      0.33      0.40         3\n",
      "        571       0.00      0.00      0.00         1\n",
      "        572       0.54      1.00      0.70         7\n",
      "        573       0.50      0.50      0.50         2\n",
      "        574       1.00      0.67      0.80         3\n",
      "        575       0.00      0.00      0.00         1\n",
      "        576       0.80      0.71      0.75        17\n",
      "        577       0.00      0.00      0.00         1\n",
      "        578       0.00      0.00      0.00         1\n",
      "        579       1.00      1.00      1.00         2\n",
      "        580       1.00      1.00      1.00         2\n",
      "        581       1.00      0.50      0.67         2\n",
      "        582       1.00      1.00      1.00         1\n",
      "        583       0.00      0.00      0.00         1\n",
      "        584       0.55      0.67      0.60         9\n",
      "        585       1.00      0.29      0.44         7\n",
      "        586       0.00      0.00      0.00         2\n",
      "        587       1.00      1.00      1.00         2\n",
      "        588       0.00      0.00      0.00         1\n",
      "        589       1.00      1.00      1.00         3\n",
      "        590       1.00      1.00      1.00         4\n",
      "        591       0.00      0.00      0.00         2\n",
      "        592       0.50      1.00      0.67         1\n",
      "        593       1.00      0.33      0.50         3\n",
      "        594       0.00      0.00      0.00         2\n",
      "        595       0.50      1.00      0.67         4\n",
      "        596       1.00      1.00      1.00         2\n",
      "        597       0.00      0.00      0.00         2\n",
      "        598       1.00      0.75      0.86         4\n",
      "        599       0.67      0.55      0.60        11\n",
      "        600       0.00      0.00      0.00         1\n",
      "        601       0.60      0.75      0.67         4\n",
      "        602       0.64      0.52      0.57        44\n",
      "        603       0.50      0.71      0.59         7\n",
      "        604       1.00      0.75      0.86         4\n",
      "        605       0.00      0.00      0.00         2\n",
      "        606       1.00      1.00      1.00         1\n",
      "        607       1.00      0.80      0.89         5\n",
      "        608       0.50      1.00      0.67         1\n",
      "        609       0.50      1.00      0.67         1\n",
      "        610       0.86      0.67      0.75         9\n",
      "        611       1.00      0.67      0.80         3\n",
      "        612       0.00      0.00      0.00         1\n",
      "        613       0.00      0.00      0.00         3\n",
      "        614       1.00      1.00      1.00         2\n",
      "        615       0.00      0.00      0.00         1\n",
      "        616       0.33      1.00      0.50         2\n",
      "        617       0.00      0.00      0.00         1\n",
      "        618       0.00      0.00      0.00         1\n",
      "        619       0.71      1.00      0.83         5\n",
      "        620       0.50      0.67      0.57         6\n",
      "        621       1.00      1.00      1.00         2\n",
      "        622       0.00      0.00      0.00         1\n",
      "        623       0.33      0.25      0.29         4\n",
      "        624       1.00      0.20      0.33         5\n",
      "        625       0.81      0.88      0.85        25\n",
      "        626       1.00      0.50      0.67         2\n",
      "        627       1.00      0.33      0.50         3\n",
      "        628       0.00      0.00      0.00         2\n",
      "        629       0.00      0.00      0.00         1\n",
      "        630       0.88      0.88      0.88         8\n",
      "        631       0.57      0.80      0.67        10\n",
      "        632       0.00      0.00      0.00         1\n",
      "        633       0.77      0.77      0.77        13\n",
      "        634       0.00      0.00      0.00         1\n",
      "        635       0.00      0.00      0.00         2\n",
      "        636       1.00      1.00      1.00         1\n",
      "        637       0.65      0.79      0.71        28\n",
      "        638       0.00      0.00      0.00         1\n",
      "        639       0.50      0.20      0.29        15\n",
      "        640       0.00      0.00      0.00         2\n",
      "        641       0.00      0.00      0.00         2\n",
      "        642       0.88      0.50      0.64        14\n",
      "        643       0.00      0.00      0.00         2\n",
      "        644       1.00      0.33      0.50         6\n",
      "        645       0.00      0.00      0.00         1\n",
      "        646       0.00      0.00      0.00         1\n",
      "        647       0.33      0.25      0.29         4\n",
      "        648       0.83      1.00      0.91         5\n",
      "        649       0.86      1.00      0.92         6\n",
      "        650       0.88      0.88      0.88         8\n",
      "        651       0.80      0.89      0.84         9\n",
      "        652       0.67      0.80      0.73         5\n",
      "        653       0.50      1.00      0.67         1\n",
      "        654       0.00      0.00      0.00         1\n",
      "        655       0.75      0.71      0.73        34\n",
      "        656       0.00      0.00      0.00         1\n",
      "        657       0.97      1.00      0.99        33\n",
      "        658       1.00      1.00      1.00         5\n",
      "        659       0.00      0.00      0.00         3\n",
      "        660       0.77      1.00      0.87        10\n",
      "        661       0.00      0.00      0.00         2\n",
      "        662       1.00      0.62      0.76        13\n",
      "        663       0.89      1.00      0.94         8\n",
      "        664       0.33      1.00      0.50         5\n",
      "        665       0.40      0.50      0.44         4\n",
      "        666       0.96      0.82      0.89        33\n",
      "        667       0.86      1.00      0.93        32\n",
      "        668       0.67      1.00      0.80         4\n",
      "        669       0.00      0.00      0.00         2\n",
      "        670       1.00      0.92      0.96        12\n",
      "        671       0.92      0.99      0.96       110\n",
      "        672       0.44      0.78      0.56         9\n",
      "        673       1.00      0.62      0.77         8\n",
      "        674       0.96      1.00      0.98        46\n",
      "        675       0.48      0.89      0.63        18\n",
      "        676       0.90      0.75      0.82        12\n",
      "        677       0.68      0.87      0.76        15\n",
      "        678       0.67      1.00      0.80         2\n",
      "        679       1.00      1.00      1.00         3\n",
      "        680       1.00      0.50      0.67         2\n",
      "        681       0.00      0.00      0.00         1\n",
      "        682       0.57      0.67      0.62         6\n",
      "        683       0.00      0.00      0.00         2\n",
      "        684       0.00      0.00      0.00         1\n",
      "        685       0.00      0.00      0.00         1\n",
      "        686       0.43      0.50      0.46         6\n",
      "        687       0.00      0.00      0.00         2\n",
      "        688       0.50      1.00      0.67         1\n",
      "        689       0.00      0.00      0.00         1\n",
      "        690       0.67      0.89      0.76         9\n",
      "        691       1.00      0.50      0.67         2\n",
      "        692       1.00      0.86      0.92         7\n",
      "        693       0.00      0.00      0.00         2\n",
      "        694       0.00      0.00      0.00         1\n",
      "        695       0.50      0.45      0.48        11\n",
      "        696       0.00      0.00      0.00         2\n",
      "        697       1.00      0.50      0.67         2\n",
      "        698       0.67      0.50      0.57         4\n",
      "        699       0.83      1.00      0.91         5\n",
      "        700       1.00      0.50      0.67         2\n",
      "        701       0.56      1.00      0.72        14\n",
      "        702       0.75      1.00      0.86         6\n",
      "        703       1.00      1.00      1.00         1\n",
      "        704       0.00      0.00      0.00         1\n",
      "        705       0.50      0.50      0.50         2\n",
      "        706       0.50      1.00      0.67         2\n",
      "        707       0.00      0.00      0.00         2\n",
      "        708       0.60      0.75      0.67         8\n",
      "        709       0.00      0.00      0.00         1\n",
      "        710       0.93      1.00      0.97        14\n",
      "        711       0.00      0.00      0.00         1\n",
      "        712       0.80      0.40      0.53        10\n",
      "        713       1.00      0.75      0.86         4\n",
      "        714       0.90      0.82      0.86        11\n",
      "        715       0.00      0.00      0.00         1\n",
      "        716       0.00      0.00      0.00         1\n",
      "        717       0.33      0.50      0.40         2\n",
      "        718       1.00      0.50      0.67         2\n",
      "        719       0.00      0.00      0.00         2\n",
      "        720       0.00      0.00      0.00         1\n",
      "        721       1.00      1.00      1.00         1\n",
      "        722       0.00      0.00      0.00         1\n",
      "        723       0.78      1.00      0.88         7\n",
      "        724       1.00      0.83      0.91         6\n",
      "        725       0.50      0.25      0.33         4\n",
      "        726       0.00      0.00      0.00         5\n",
      "        727       0.00      0.00      0.00         3\n",
      "        728       0.00      0.00      0.00         3\n",
      "        729       0.25      0.12      0.17         8\n",
      "        730       0.73      0.65      0.69        17\n",
      "        731       1.00      0.64      0.78        11\n",
      "        732       0.57      1.00      0.73         4\n",
      "        733       0.00      0.00      0.00         1\n",
      "        734       0.00      0.00      0.00         4\n",
      "        735       0.00      0.00      0.00         1\n",
      "        736       0.00      0.00      0.00         1\n",
      "        737       0.25      1.00      0.40         1\n",
      "        738       0.00      0.00      0.00         2\n",
      "        739       0.00      0.00      0.00         1\n",
      "        740       0.50      0.65      0.57        23\n",
      "        741       0.78      0.78      0.78         9\n",
      "        742       0.80      0.80      0.80         5\n",
      "        743       0.33      1.00      0.50         1\n",
      "        744       0.00      0.00      0.00         4\n",
      "        745       0.68      0.70      0.69        40\n",
      "        746       0.46      0.35      0.40        17\n",
      "        747       1.00      0.25      0.40         4\n",
      "        748       0.50      1.00      0.67         1\n",
      "        749       0.00      0.00      0.00         1\n",
      "        750       0.00      0.00      0.00         1\n",
      "        751       0.40      0.67      0.50         3\n",
      "        752       1.00      0.67      0.80         6\n",
      "        753       0.38      0.60      0.46         5\n",
      "        754       0.00      0.00      0.00         1\n",
      "        755       0.50      1.00      0.67         1\n",
      "        756       0.69      0.82      0.75        11\n",
      "        757       0.46      0.40      0.43        15\n",
      "        758       0.50      0.50      0.50         4\n",
      "        759       0.17      0.25      0.20         4\n",
      "        760       0.00      0.00      0.00         2\n",
      "        761       0.60      0.55      0.57        11\n",
      "        762       0.75      1.00      0.86         3\n",
      "        763       1.00      1.00      1.00         8\n",
      "        764       0.00      0.00      0.00         1\n",
      "        765       0.00      0.00      0.00         2\n",
      "        766       0.67      0.50      0.57         4\n",
      "        767       0.33      0.50      0.40         2\n",
      "        768       0.00      0.00      0.00         4\n",
      "        769       0.17      1.00      0.29         1\n",
      "        770       0.80      0.92      0.86        13\n",
      "        771       0.71      0.83      0.77         6\n",
      "        772       1.00      1.00      1.00         1\n",
      "        773       0.00      0.00      0.00         3\n",
      "        774       0.00      0.00      0.00         1\n",
      "        775       0.71      0.71      0.71         7\n",
      "        776       1.00      0.93      0.96        27\n",
      "        777       0.00      0.00      0.00         1\n",
      "        778       0.00      0.00      0.00         1\n",
      "        779       1.00      1.00      1.00         4\n",
      "        780       0.00      0.00      0.00         3\n",
      "        781       0.67      0.40      0.50         5\n",
      "        782       0.90      0.69      0.78        13\n",
      "        783       0.44      0.25      0.32        16\n",
      "        784       0.84      0.96      0.89        93\n",
      "        785       1.00      1.00      1.00         2\n",
      "        786       0.50      0.25      0.33         4\n",
      "        787       0.00      0.00      0.00         2\n",
      "        788       0.00      0.00      0.00         1\n",
      "        789       0.00      0.00      0.00         1\n",
      "        790       1.00      1.00      1.00         2\n",
      "        791       0.33      0.33      0.33         3\n",
      "        792       0.36      0.40      0.38        10\n",
      "        793       1.00      0.25      0.40         4\n",
      "        794       0.43      0.75      0.55         4\n",
      "        795       1.00      0.75      0.86         4\n",
      "        796       0.25      0.50      0.33         2\n",
      "        797       0.63      0.63      0.63        38\n",
      "        798       0.00      0.00      0.00         2\n",
      "        799       0.00      0.00      0.00         2\n",
      "        800       0.00      0.00      0.00         1\n",
      "        801       0.77      0.85      0.81        59\n",
      "        802       0.83      0.71      0.77         7\n",
      "        803       1.00      0.33      0.50         3\n",
      "        804       0.00      0.00      0.00         1\n",
      "        805       0.00      0.00      0.00         1\n",
      "        806       0.00      0.00      0.00         1\n",
      "        807       0.00      0.00      0.00         2\n",
      "        808       1.00      1.00      1.00         1\n",
      "        809       1.00      1.00      1.00         2\n",
      "        810       0.00      0.00      0.00         3\n",
      "        811       0.50      1.00      0.67         4\n",
      "        812       1.00      1.00      1.00         1\n",
      "        813       0.44      0.57      0.50         7\n",
      "        814       0.00      0.00      0.00         1\n",
      "        815       1.00      0.67      0.80         3\n",
      "        816       0.20      0.29      0.24         7\n",
      "        817       1.00      1.00      1.00         2\n",
      "        818       1.00      0.60      0.75         5\n",
      "        819       1.00      1.00      1.00         2\n",
      "        820       1.00      0.50      0.67         2\n",
      "        821       0.88      0.78      0.82         9\n",
      "        822       1.00      0.67      0.80         3\n",
      "        823       0.45      0.90      0.60        10\n",
      "        824       0.75      0.50      0.60         6\n",
      "        825       0.14      0.33      0.20         3\n",
      "        826       0.00      0.00      0.00         2\n",
      "        827       0.00      0.00      0.00         3\n",
      "        828       0.00      0.00      0.00         2\n",
      "        829       0.00      0.00      0.00         2\n",
      "        830       0.00      0.00      0.00         2\n",
      "        831       0.50      0.67      0.57         3\n",
      "        832       0.00      0.00      0.00         3\n",
      "        833       0.67      0.62      0.65        16\n",
      "        834       0.33      0.50      0.40         2\n",
      "        835       0.00      0.00      0.00         1\n",
      "        836       0.83      0.71      0.77         7\n",
      "        837       0.50      0.50      0.50         2\n",
      "        838       0.00      0.00      0.00         1\n",
      "        839       0.25      0.20      0.22         5\n",
      "        840       0.00      0.00      0.00         2\n",
      "        841       0.67      0.67      0.67        15\n",
      "        842       1.00      0.50      0.67         2\n",
      "        843       0.00      0.00      0.00         1\n",
      "        844       0.53      0.81      0.64        26\n",
      "        845       0.00      0.00      0.00         1\n",
      "        846       0.00      0.00      0.00         3\n",
      "        847       0.00      0.00      0.00         1\n",
      "        848       0.00      0.00      0.00         2\n",
      "        849       0.71      0.83      0.77         6\n",
      "        850       0.00      0.00      0.00         1\n",
      "        851       0.00      0.00      0.00         2\n",
      "        852       0.00      0.00      0.00         2\n",
      "        853       0.25      0.50      0.33         2\n",
      "        854       1.00      1.00      1.00         3\n",
      "        855       0.00      0.00      0.00        11\n",
      "        856       0.50      0.50      0.50         2\n",
      "        857       0.00      0.00      0.00         1\n",
      "        858       0.00      0.00      0.00         3\n",
      "        859       0.14      0.12      0.13         8\n",
      "        860       0.00      0.00      0.00         1\n",
      "        861       0.00      0.00      0.00         3\n",
      "        862       0.46      1.00      0.63         6\n",
      "        863       0.00      0.00      0.00         1\n",
      "        864       0.40      0.67      0.50         3\n",
      "        865       0.00      0.00      0.00         1\n",
      "        866       0.00      0.00      0.00         5\n",
      "        867       0.63      0.73      0.68        33\n",
      "        868       0.50      0.67      0.57         3\n",
      "        869       0.00      0.00      0.00         3\n",
      "        870       0.33      0.25      0.29         4\n",
      "        871       0.00      0.00      0.00         1\n",
      "        872       0.00      0.00      0.00         2\n",
      "        873       1.00      0.50      0.67         2\n",
      "        874       0.89      0.94      0.92        18\n",
      "        875       1.00      0.57      0.73         7\n",
      "        876       0.50      0.50      0.50         2\n",
      "        877       0.00      0.00      0.00         2\n",
      "        878       0.22      1.00      0.36         2\n",
      "        879       0.00      0.00      0.00         1\n",
      "        880       0.00      0.00      0.00         2\n",
      "        881       0.62      1.00      0.77         5\n",
      "        882       0.31      0.57      0.40         7\n",
      "        883       0.00      0.00      0.00         1\n",
      "        884       0.33      1.00      0.50         1\n",
      "        885       0.79      1.00      0.88        11\n",
      "        886       0.00      0.00      0.00         3\n",
      "        887       0.00      0.00      0.00         1\n",
      "        888       0.79      0.79      0.79        19\n",
      "        889       1.00      0.33      0.50         3\n",
      "        890       1.00      0.80      0.89         5\n",
      "        891       1.00      1.00      1.00         1\n",
      "        892       0.67      0.33      0.44         6\n",
      "        893       0.75      1.00      0.86         3\n",
      "        894       1.00      1.00      1.00         1\n",
      "        895       1.00      0.50      0.67         2\n",
      "        896       0.00      0.00      0.00         1\n",
      "        897       0.00      0.00      0.00         1\n",
      "        898       0.67      1.00      0.80         2\n",
      "        899       0.00      0.00      0.00         1\n",
      "        900       0.50      0.50      0.50         2\n",
      "        901       0.00      0.00      0.00         1\n",
      "        902       0.00      0.00      0.00         1\n",
      "        903       0.00      0.00      0.00         2\n",
      "        904       1.00      1.00      1.00         1\n",
      "        905       0.00      0.00      0.00         1\n",
      "        906       0.00      0.00      0.00         8\n",
      "        907       0.29      0.29      0.29         7\n",
      "        908       0.00      0.00      0.00         2\n",
      "        909       0.00      0.00      0.00         1\n",
      "        910       0.00      0.00      0.00         3\n",
      "        911       0.00      0.00      0.00         1\n",
      "        912       0.29      0.80      0.42         5\n",
      "        913       0.00      0.00      0.00         2\n",
      "        914       0.00      0.00      0.00         1\n",
      "        915       0.00      0.00      0.00         3\n",
      "        916       0.00      0.00      0.00         1\n",
      "        917       0.00      0.00      0.00         1\n",
      "        918       0.00      0.00      0.00         1\n",
      "        919       0.00      0.00      0.00         1\n",
      "        920       1.00      0.50      0.67         2\n",
      "        921       0.50      1.00      0.67         1\n",
      "        922       0.00      0.00      0.00         1\n",
      "        923       0.00      0.00      0.00         5\n",
      "        924       0.12      0.33      0.18         3\n",
      "        925       0.00      0.00      0.00         1\n",
      "        926       0.00      0.00      0.00         1\n",
      "        927       0.00      0.00      0.00         1\n",
      "        928       1.00      1.00      1.00         5\n",
      "        929       0.44      0.80      0.57         5\n",
      "        930       0.00      0.00      0.00         1\n",
      "        931       0.00      0.00      0.00         4\n",
      "        932       0.00      0.00      0.00         1\n",
      "        933       0.88      0.97      0.92        88\n",
      "        934       0.67      0.96      0.79        23\n",
      "        935       0.00      0.00      0.00         3\n",
      "        936       0.00      0.00      0.00         2\n",
      "        937       1.00      0.62      0.77         8\n",
      "        938       0.00      0.00      0.00         2\n",
      "        939       0.00      0.00      0.00         1\n",
      "        940       0.00      0.00      0.00         1\n",
      "        941       1.00      0.50      0.67         2\n",
      "        942       1.00      0.50      0.67         2\n",
      "        943       0.50      0.50      0.50         2\n",
      "        944       0.89      0.79      0.83        61\n",
      "        945       0.59      0.92      0.72        26\n",
      "        946       1.00      0.25      0.40         4\n",
      "        947       0.00      0.00      0.00         1\n",
      "        948       0.50      0.80      0.62         5\n",
      "        949       0.00      0.00      0.00         2\n",
      "        950       0.00      0.00      0.00         2\n",
      "        951       0.00      0.00      0.00         1\n",
      "        952       0.00      0.00      0.00         2\n",
      "        953       0.80      0.67      0.73         6\n",
      "        954       0.00      0.00      0.00         1\n",
      "        955       0.56      1.00      0.71         5\n",
      "        956       0.50      0.12      0.20         8\n",
      "        957       0.00      0.00      0.00         1\n",
      "        958       1.00      1.00      1.00         6\n",
      "        959       1.00      1.00      1.00         3\n",
      "        960       0.77      0.77      0.77        13\n",
      "        961       0.91      1.00      0.95        10\n",
      "        962       0.00      0.00      0.00         1\n",
      "        963       0.00      0.00      0.00         1\n",
      "        964       0.00      0.00      0.00         1\n",
      "        965       0.67      1.00      0.80         2\n",
      "        966       0.00      0.00      0.00         3\n",
      "        967       0.00      0.00      0.00         1\n",
      "        968       0.00      0.00      0.00         3\n",
      "        969       0.61      0.92      0.73        12\n",
      "        970       0.00      0.00      0.00         1\n",
      "        971       1.00      1.00      1.00         6\n",
      "        972       1.00      0.20      0.33         5\n",
      "        973       0.40      0.29      0.33         7\n",
      "        974       0.42      0.83      0.56        12\n",
      "        975       1.00      0.33      0.50         3\n",
      "        976       1.00      0.88      0.93         8\n",
      "        977       0.75      0.43      0.55         7\n",
      "        978       0.00      0.00      0.00         4\n",
      "        979       0.00      0.00      0.00         2\n",
      "        980       0.00      0.00      0.00         1\n",
      "        981       0.67      0.50      0.57         4\n",
      "        982       0.75      0.75      0.75         4\n",
      "        983       0.80      0.40      0.53        10\n",
      "        984       0.85      0.69      0.76        16\n",
      "        985       0.60      1.00      0.75         3\n",
      "        986       0.00      0.00      0.00         1\n",
      "        987       0.75      1.00      0.86         3\n",
      "        988       0.67      0.67      0.67         6\n",
      "        989       0.00      0.00      0.00         1\n",
      "        990       1.00      0.67      0.80         3\n",
      "        991       1.00      0.96      0.98        24\n",
      "        992       0.90      1.00      0.95        18\n",
      "        993       1.00      0.50      0.67         2\n",
      "        994       0.92      1.00      0.96        22\n",
      "        995       0.00      0.00      0.00         1\n",
      "        996       0.50      0.17      0.25         6\n",
      "        997       0.63      0.67      0.65        18\n",
      "        998       0.60      1.00      0.75         3\n",
      "        999       1.00      0.67      0.80         3\n",
      "       1000       1.00      1.00      1.00         9\n",
      "       1001       0.00      0.00      0.00         2\n",
      "       1002       0.85      0.98      0.91       213\n",
      "       1003       0.70      0.78      0.74         9\n",
      "       1004       0.33      0.33      0.33         3\n",
      "       1005       1.00      0.80      0.89         5\n",
      "       1006       0.00      0.00      0.00         1\n",
      "       1007       0.93      0.76      0.84        17\n",
      "       1008       0.00      0.00      0.00         1\n",
      "       1009       1.00      1.00      1.00         1\n",
      "       1010       1.00      0.67      0.80         3\n",
      "       1011       0.00      0.00      0.00         6\n",
      "       1012       1.00      0.57      0.73         7\n",
      "       1013       0.50      0.33      0.40         3\n",
      "       1014       0.00      0.00      0.00         1\n",
      "       1015       0.50      1.00      0.67         1\n",
      "       1016       0.33      1.00      0.50         1\n",
      "       1017       0.60      0.75      0.67         8\n",
      "       1018       0.50      0.17      0.25         6\n",
      "       1019       0.71      0.83      0.77        12\n",
      "       1020       0.67      1.00      0.80         2\n",
      "       1021       0.86      0.75      0.80         8\n",
      "       1022       1.00      0.50      0.67         2\n",
      "       1023       1.00      1.00      1.00         4\n",
      "       1024       0.50      1.00      0.67         2\n",
      "       1025       1.00      0.75      0.86        12\n",
      "       1026       0.89      0.94      0.92        36\n",
      "       1027       0.00      0.00      0.00         4\n",
      "       1028       0.00      0.00      0.00         1\n",
      "       1029       0.80      0.57      0.67         7\n",
      "       1030       0.00      0.00      0.00         1\n",
      "       1031       0.80      0.89      0.84         9\n",
      "       1032       1.00      1.00      1.00         4\n",
      "       1033       1.00      1.00      1.00         1\n",
      "       1034       1.00      1.00      1.00         1\n",
      "       1035       1.00      0.86      0.92         7\n",
      "       1036       1.00      0.86      0.92         7\n",
      "       1037       0.94      0.88      0.91        33\n",
      "       1038       0.62      0.73      0.67        11\n",
      "       1039       0.67      1.00      0.80        12\n",
      "       1040       0.81      0.66      0.72        38\n",
      "       1041       0.00      0.00      0.00         2\n",
      "       1042       1.00      0.50      0.67         2\n",
      "       1043       0.75      0.75      0.75         4\n",
      "       1044       0.86      0.40      0.55        15\n",
      "       1045       0.38      0.60      0.46         5\n",
      "       1046       0.40      0.40      0.40         5\n",
      "       1047       0.55      0.50      0.52        22\n",
      "       1048       1.00      0.79      0.88        14\n",
      "       1049       0.33      0.33      0.33         3\n",
      "       1050       0.00      0.00      0.00         3\n",
      "       1051       0.67      1.00      0.80         2\n",
      "       1052       0.60      0.38      0.46         8\n",
      "       1053       0.00      0.00      0.00         1\n",
      "       1054       0.88      0.88      0.88        26\n",
      "       1055       1.00      0.88      0.93         8\n",
      "       1056       0.65      0.69      0.67        52\n",
      "       1057       0.86      0.86      0.86        28\n",
      "       1058       0.00      0.00      0.00         2\n",
      "       1059       0.33      0.50      0.40         2\n",
      "       1060       0.38      0.60      0.46         5\n",
      "       1061       0.50      1.00      0.67         2\n",
      "       1062       0.75      0.75      0.75         4\n",
      "       1063       1.00      0.58      0.74        12\n",
      "       1064       0.00      0.00      0.00         1\n",
      "       1065       0.83      0.77      0.80        13\n",
      "       1066       0.00      0.00      0.00         1\n",
      "       1067       0.95      0.83      0.89        24\n",
      "       1068       0.83      0.78      0.81        32\n",
      "       1069       0.00      0.00      0.00         1\n",
      "       1070       1.00      1.00      1.00         1\n",
      "       1071       0.70      0.64      0.67        11\n",
      "       1072       0.00      0.00      0.00         1\n",
      "       1073       0.83      1.00      0.91         5\n",
      "       1074       0.00      0.00      0.00         2\n",
      "       1075       0.56      0.62      0.59         8\n",
      "       1076       1.00      1.00      1.00         3\n",
      "       1077       0.86      0.99      0.92       132\n",
      "       1078       0.69      0.61      0.65        18\n",
      "       1079       0.91      0.67      0.77        43\n",
      "       1080       1.00      0.75      0.86         4\n",
      "       1081       0.00      0.00      0.00         1\n",
      "       1082       1.00      0.25      0.40         4\n",
      "       1083       0.00      0.00      0.00         2\n",
      "       1084       1.00      0.67      0.80         3\n",
      "       1085       0.00      0.00      0.00         1\n",
      "       1086       1.00      1.00      1.00         1\n",
      "       1087       0.56      0.50      0.53        10\n",
      "       1088       1.00      1.00      1.00        10\n",
      "       1089       0.00      0.00      0.00         1\n",
      "       1090       0.50      0.50      0.50         2\n",
      "       1091       0.00      0.00      0.00         1\n",
      "       1092       0.50      0.25      0.33         4\n",
      "       1093       0.72      0.87      0.79       175\n",
      "       1094       0.50      0.25      0.33         4\n",
      "       1095       1.00      1.00      1.00         1\n",
      "       1096       0.00      0.00      0.00         3\n",
      "       1097       0.00      0.00      0.00         2\n",
      "       1098       0.00      0.00      0.00         2\n",
      "       1099       0.83      0.90      0.86        21\n",
      "       1100       0.69      0.85      0.76       177\n",
      "       1101       1.00      0.73      0.85        15\n",
      "       1102       0.62      0.65      0.64        23\n",
      "       1103       0.00      0.00      0.00         4\n",
      "       1104       0.00      0.00      0.00         1\n",
      "       1105       0.00      0.00      0.00         1\n",
      "       1106       0.00      0.00      0.00         2\n",
      "       1107       0.50      0.50      0.50         2\n",
      "       1108       0.00      0.00      0.00         2\n",
      "       1109       0.40      1.00      0.57         2\n",
      "       1110       0.00      0.00      0.00         2\n",
      "       1111       0.00      0.00      0.00         3\n",
      "       1112       0.00      0.00      0.00         4\n",
      "       1113       0.69      0.78      0.73        23\n",
      "       1114       0.47      0.67      0.55        12\n",
      "       1115       0.00      0.00      0.00         2\n",
      "       1116       0.00      0.00      0.00         2\n",
      "       1117       0.75      0.86      0.80         7\n",
      "       1118       0.80      0.89      0.84         9\n",
      "       1119       1.00      0.50      0.67         2\n",
      "       1120       1.00      1.00      1.00         1\n",
      "       1121       0.90      1.00      0.95        43\n",
      "       1122       1.00      0.67      0.80         6\n",
      "       1123       1.00      1.00      1.00         1\n",
      "       1124       1.00      0.29      0.44         7\n",
      "       1125       0.70      0.67      0.68        21\n",
      "       1126       0.00      0.00      0.00         1\n",
      "       1127       0.92      0.92      0.92        13\n",
      "       1128       0.74      0.94      0.83        18\n",
      "       1129       0.75      1.00      0.86         3\n",
      "       1130       0.00      0.00      0.00         1\n",
      "       1131       1.00      1.00      1.00         1\n",
      "       1132       0.00      0.00      0.00         1\n",
      "       1133       1.00      0.25      0.40         4\n",
      "       1134       1.00      1.00      1.00         1\n",
      "       1135       0.00      0.00      0.00         2\n",
      "       1136       1.00      1.00      1.00         2\n",
      "       1137       0.93      0.87      0.90        15\n",
      "       1138       0.75      0.70      0.73        47\n",
      "       1139       0.00      0.00      0.00         1\n",
      "       1140       0.00      0.00      0.00         3\n",
      "       1141       0.62      1.00      0.77         5\n",
      "       1142       0.80      0.89      0.84         9\n",
      "       1143       0.00      0.00      0.00         1\n",
      "       1144       0.91      0.91      0.91        46\n",
      "       1145       1.00      0.43      0.60         7\n",
      "       1146       1.00      1.00      1.00         1\n",
      "       1147       0.00      0.00      0.00         3\n",
      "       1148       1.00      0.33      0.50         3\n",
      "       1149       1.00      0.83      0.91         6\n",
      "       1150       1.00      0.67      0.80         6\n",
      "       1151       0.00      0.00      0.00         2\n",
      "       1152       0.00      0.00      0.00         3\n",
      "       1153       0.54      0.72      0.62        18\n",
      "       1154       0.00      0.00      0.00         2\n",
      "       1155       0.33      0.17      0.22         6\n",
      "       1156       0.00      0.00      0.00         1\n",
      "       1157       0.50      0.50      0.50         4\n",
      "       1158       0.50      0.67      0.57         3\n",
      "       1159       0.00      0.00      0.00         2\n",
      "       1160       0.00      0.00      0.00         1\n",
      "       1161       0.33      0.40      0.36         5\n",
      "       1162       0.00      0.00      0.00         1\n",
      "       1163       0.00      0.00      0.00         1\n",
      "       1164       1.00      0.80      0.89         5\n",
      "       1165       0.00      0.00      0.00         1\n",
      "       1166       0.00      0.00      0.00         3\n",
      "       1167       1.00      0.67      0.80         3\n",
      "       1168       0.00      0.00      0.00         2\n",
      "       1169       0.00      0.00      0.00         3\n",
      "       1170       0.00      0.00      0.00         2\n",
      "       1171       0.00      0.00      0.00         1\n",
      "       1172       0.00      0.00      0.00         1\n",
      "       1173       0.00      0.00      0.00         1\n",
      "       1174       0.92      0.86      0.89        14\n",
      "       1175       0.00      0.00      0.00         1\n",
      "       1176       0.61      0.76      0.68        33\n",
      "       1177       0.00      0.00      0.00         3\n",
      "       1178       0.77      0.68      0.72        34\n",
      "       1179       1.00      0.75      0.86         4\n",
      "       1180       0.00      0.00      0.00         1\n",
      "       1181       0.00      0.00      0.00         1\n",
      "       1182       0.67      1.00      0.80         6\n",
      "       1183       1.00      1.00      1.00         1\n",
      "       1184       0.00      0.00      0.00         8\n",
      "       1185       1.00      0.33      0.50         6\n",
      "       1186       1.00      0.50      0.67         2\n",
      "       1187       0.50      1.00      0.67         1\n",
      "       1188       0.50      1.00      0.67         1\n",
      "       1189       0.50      1.00      0.67         7\n",
      "       1190       0.00      0.00      0.00         1\n",
      "       1191       0.85      0.92      0.88        12\n",
      "       1192       1.00      1.00      1.00         1\n",
      "       1193       1.00      0.50      0.67         2\n",
      "       1194       0.73      0.92      0.81        12\n",
      "       1195       1.00      0.33      0.50         3\n",
      "       1196       1.00      1.00      1.00         4\n",
      "       1197       0.94      0.88      0.91        17\n",
      "       1198       0.67      0.67      0.67         3\n",
      "       1199       1.00      1.00      1.00         2\n",
      "       1200       0.00      0.00      0.00         1\n",
      "       1201       0.00      0.00      0.00         1\n",
      "       1202       0.00      0.00      0.00         1\n",
      "       1203       1.00      1.00      1.00         5\n",
      "       1204       0.00      0.00      0.00         1\n",
      "       1205       1.00      1.00      1.00         5\n",
      "       1206       1.00      1.00      1.00         5\n",
      "       1207       0.00      0.00      0.00         3\n",
      "       1208       0.00      0.00      0.00         1\n",
      "       1209       0.00      0.00      0.00         1\n",
      "       1210       0.44      0.67      0.53         6\n",
      "       1211       0.00      0.00      0.00         2\n",
      "       1212       0.33      0.50      0.40         2\n",
      "       1213       1.00      0.50      0.67         2\n",
      "       1214       0.00      0.00      0.00         1\n",
      "       1215       0.40      0.33      0.36         6\n",
      "       1216       0.00      0.00      0.00         1\n",
      "       1217       0.67      0.67      0.67         3\n",
      "       1218       0.00      0.00      0.00         1\n",
      "       1219       0.50      1.00      0.67         1\n",
      "       1220       0.78      0.90      0.84        51\n",
      "       1221       0.00      0.00      0.00         1\n",
      "       1222       0.96      0.92      0.94        26\n",
      "       1223       0.00      0.00      0.00         1\n",
      "       1224       0.00      0.00      0.00         1\n",
      "       1225       1.00      0.67      0.80         6\n",
      "       1226       0.00      0.00      0.00         2\n",
      "       1227       0.72      0.72      0.72        29\n",
      "       1228       0.00      0.00      0.00         1\n",
      "       1229       0.83      0.83      0.83         6\n",
      "       1230       0.89      0.80      0.84        10\n",
      "       1231       0.00      0.00      0.00         1\n",
      "       1232       0.25      0.33      0.29         3\n",
      "       1233       0.00      0.00      0.00         1\n",
      "       1234       0.00      0.00      0.00         1\n",
      "       1235       0.00      0.00      0.00         1\n",
      "       1236       1.00      0.62      0.77         8\n",
      "       1237       1.00      0.75      0.86         4\n",
      "       1238       0.00      0.00      0.00         1\n",
      "       1239       0.40      1.00      0.57         4\n",
      "       1240       1.00      0.50      0.67         2\n",
      "       1241       1.00      1.00      1.00         8\n",
      "       1242       0.20      0.14      0.17         7\n",
      "       1243       0.50      0.33      0.40         3\n",
      "       1244       0.85      1.00      0.92        29\n",
      "       1245       0.33      0.17      0.22         6\n",
      "       1246       0.75      0.60      0.67         5\n",
      "       1247       0.90      0.95      0.92        19\n",
      "       1248       0.75      0.50      0.60         6\n",
      "       1249       0.00      0.00      0.00         2\n",
      "       1250       1.00      1.00      1.00         3\n",
      "       1251       0.00      0.00      0.00         3\n",
      "       1252       1.00      1.00      1.00         3\n",
      "       1253       0.00      0.00      0.00         1\n",
      "       1254       0.66      0.56      0.61        48\n",
      "       1255       0.92      1.00      0.96        12\n",
      "       1256       0.00      0.00      0.00         1\n",
      "       1257       0.00      0.00      0.00         2\n",
      "       1258       0.40      1.00      0.57         2\n",
      "       1259       1.00      1.00      1.00         9\n",
      "       1260       1.00      1.00      1.00        13\n",
      "       1261       0.00      0.00      0.00         1\n",
      "       1262       0.33      0.50      0.40         6\n",
      "       1263       0.50      0.67      0.57        12\n",
      "       1264       0.95      0.96      0.95       202\n",
      "       1265       0.93      0.94      0.94       210\n",
      "       1266       0.95      0.95      0.95        22\n",
      "       1267       0.85      0.80      0.82        84\n",
      "       1268       1.00      0.93      0.97        15\n",
      "       1269       0.71      0.40      0.51        30\n",
      "       1270       1.00      1.00      1.00         4\n",
      "       1271       0.33      0.12      0.18         8\n",
      "       1272       0.00      0.00      0.00         1\n",
      "       1273       0.64      0.77      0.70        75\n",
      "       1274       0.91      0.94      0.92       125\n",
      "       1275       0.00      0.00      0.00         2\n",
      "       1276       0.00      0.00      0.00         1\n",
      "       1277       0.50      0.50      0.50         4\n",
      "       1278       0.00      0.00      0.00         1\n",
      "       1279       0.00      0.00      0.00         2\n",
      "       1280       0.00      0.00      0.00         1\n",
      "       1281       0.96      1.00      0.98        22\n",
      "       1282       0.75      0.50      0.60        12\n",
      "       1283       0.87      0.93      0.90        14\n",
      "       1284       0.00      0.00      0.00         2\n",
      "       1285       0.00      0.00      0.00         1\n",
      "       1286       0.59      0.52      0.55        94\n",
      "       1287       1.00      1.00      1.00         7\n",
      "       1288       0.00      0.00      0.00         1\n",
      "       1289       0.67      0.50      0.57         4\n",
      "       1290       0.25      0.25      0.25         4\n",
      "       1291       0.00      0.00      0.00         1\n",
      "       1292       1.00      1.00      1.00         2\n",
      "       1293       0.60      0.53      0.56        17\n",
      "       1294       0.39      0.78      0.52         9\n",
      "       1295       0.00      0.00      0.00         2\n",
      "       1296       0.80      1.00      0.89         4\n",
      "       1297       1.00      0.86      0.92         7\n",
      "       1298       0.00      0.00      0.00         2\n",
      "       1299       0.00      0.00      0.00         3\n",
      "       1300       0.00      0.00      0.00         1\n",
      "       1301       0.50      0.18      0.27        11\n",
      "       1302       0.78      0.86      0.82        21\n",
      "       1303       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.78      0.79      0.78     18486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enxie/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "模型在验证集上的性能评估(准确率，召回率，f1指标和样本数)\n",
    "'''\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# print(classification_report_imbalanced(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c1cd21f672e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mebay_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ebay_category_      :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mebay_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_finn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'leaf_categ_name==\"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebay_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "随机从X_test中选择第i个样本做预测，直观的看结果\n",
    "'''\n",
    "\n",
    "i=15\n",
    "ebay_real = X_test[i:i+1]\n",
    "print(\"ebay_category_      :\",ebay_real[0])\n",
    "real = df_finn.query('leaf_categ_name==\"{}\"'.format(ebay_real[0])).values[0][1]\n",
    "print(\"gg_category_real_   :\",real)\n",
    "index = pipeline.predict(ebay_real)[0]\n",
    "print(\"gg_category_predict_:\",d[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
