{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument \n",
    "from gensim.models import Doc2Vec\n",
    "import numpy\n",
    "import random\n",
    "import os\n",
    "import nltk.stem as stem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(TaggedDocument (utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "rootPath='../data/dataset/'\n",
    "# sources = {\n",
    "#             rootPath + 'Animals & Pet Supplies_new.txt':'TRAIN_1', \n",
    "#             rootPath + 'Apparel & Accessories_new.txt':'TRAIN_2',\n",
    "#             rootPath + 'Arts & Entertainment_new.txt':'TRAIN_3',\n",
    "#             rootPath + 'Baby & Toddler_new.txt':'TRAIN_4',\n",
    "#             rootPath + 'Business & Industrial_new.txt':'TRAIN_5',\n",
    "#             rootPath + 'Cameras & Optics_new.txt':'TRAIN_6',\n",
    "#             rootPath + 'Electronics_new.txt':'TRAIN_7',\n",
    "#             rootPath + 'Food, Beverages & Tobacco_new.txt':'TRAIN_8',\n",
    "#             rootPath + 'Furniture_new.txt':'TRAIN_9',\n",
    "#             rootPath + 'Hardware_new.txt':'TRAIN_10',\n",
    "#             rootPath + 'Health & Beauty_new.txt':'TRAIN_11',\n",
    "#             rootPath + 'Home & Garden_new.txt':'TRAIN_12',\n",
    "#             rootPath + 'Luggage & Bags_new.txt':'TRAIN_13',\n",
    "#             rootPath + 'Mature_new.txt':'TRAIN_14',\n",
    "#             rootPath + 'Media_new.txt':'TRAIN_15',\n",
    "#             rootPath + 'Office Supplies_new.txt':'TRAIN_16',\n",
    "#             rootPath + 'Religious & Ceremonial_new.txt':'TRAIN_17',\n",
    "#             rootPath + 'Software_new.txt':'TRAIN_18',\n",
    "#             rootPath + 'Sporting Goods_new.txt':'TRAIN_19',\n",
    "#             rootPath + 'Toys & Games_new.txt':'TRAIN_20',\n",
    "#             rootPath + 'Vehicles & Parts_new.txt':'TRAIN_21'\n",
    "#           }\n",
    "sources = {\n",
    "            rootPath + 'Animals & Pet Supplies_new.txt':'TRAIN_1', \n",
    "            rootPath + 'Apparel & Accessories_new.txt':'TRAIN_2',\n",
    "            rootPath + 'Arts & Entertainment_new.txt':'TRAIN_3',\n",
    "            rootPath + 'Business & Industrial_new.txt':'TRAIN_4',\n",
    "            rootPath + 'Cameras & Optics_new.txt':'TRAIN_5',\n",
    "            rootPath + 'Electronics_new.txt':'TRAIN_6',\n",
    "            rootPath + 'Food, Beverages & Tobacco_new.txt':'TRAIN_7',\n",
    "            rootPath + 'Health & Beauty_new.txt':'TRAIN_8',\n",
    "            rootPath + 'Home & Garden_new.txt':'TRAIN_9',\n",
    "            rootPath + 'Mature_new.txt':'TRAIN_10',\n",
    "            rootPath + 'Media_new.txt':'TRAIN_11',\n",
    "            rootPath + 'Office Supplies_new.txt':'TRAIN_12',\n",
    "            rootPath + 'Religious & Ceremonial_new.txt':'TRAIN_13',\n",
    "            rootPath + 'Software_new.txt':'TRAIN_14',\n",
    "            rootPath + 'Sporting Goods_new.txt':'TRAIN_15',\n",
    "            rootPath + 'Toys & Games_new.txt':'TRAIN_16',\n",
    "          }\n",
    "#sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size=200\n",
    "model = Doc2Vec(min_count=1, window=10, size=feature_size, sample=1e-4, negative=5, workers=8)\n",
    "# model = Doc2Vec(min_count=1, window=10, size=feature_size, sample=1e-4, hs=1, workers=8)\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12388471"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36872"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#总样本量\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42974174, -0.84499753,  0.38913134, -0.07190589,  0.19583772,\n",
       "       -0.21709339,  0.36377141,  0.95978892, -0.7192595 , -0.37560827,\n",
       "       -0.176626  ,  0.26879093,  0.31063902, -0.6518997 , -0.26445186,\n",
       "        0.75989532,  0.56949019, -0.27620557,  0.93298906, -0.31902829], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN_1的第一条记录 的 特征\n",
    "model.docvecs['TRAIN_20_503']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "\n",
    "### Training Vectors\n",
    "\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative).\n",
    "\n",
    "Hence, we create a `numpy` array (since the classifier we use only takes numpy arrays. There are two parallel arrays, one containing the vectors (`train_arrays`) and the other containing the labels (`train_labels`).\n",
    "\n",
    "We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((model.corpus_count, feature_size))\n",
    "train_labels = numpy.zeros(model.corpus_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sources = {v:k for k,v in sources.items()} \n",
    "#new_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 429\n",
      "429 6241\n",
      "6241 16014\n",
      "16014 19560\n",
      "19560 20172\n",
      "20172 24614\n",
      "24614 24794\n",
      "24794 26469\n",
      "26469 29754\n",
      "29754 29788\n",
      "29788 30252\n",
      "30252 30575\n",
      "30575 30606\n",
      "30606 30956\n",
      "30956 34579\n",
      "34579 36872\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "label_index = 0\n",
    "for i in range(1,len(sources)+1,1):\n",
    "    prefix_train_pos = 'TRAIN_' + str(i) + '_'\n",
    "    count = len(open(new_sources['TRAIN_'+str(i)]).readlines())\n",
    "    print(index,index+count)\n",
    "    for j in range(index,index+count,1):\n",
    "        #print(prefix_train_pos+str(j))\n",
    "        train_arrays[j] = model.docvecs[prefix_train_pos+str(j-index)]\n",
    "        train_labels[j] = label_index\n",
    "    index = index+count\n",
    "    label_index =label_index+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import pprint\n",
    "# pprint.pprint(sorted(Counter(Y).items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0623354  -0.04678584 -0.00510533 -0.01090491 -0.0113547   0.01019038\n",
      " -0.09172204 -0.09480663  0.05127002 -0.08035768]\n",
      "[-0.0623354  -0.04678584 -0.00510533 -0.01090491 -0.0113547   0.01019038\n",
      " -0.09172204 -0.09480663  0.05127002 -0.08035768]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs['TRAIN_10_3'][:10])\n",
    "print(train_arrays[29757][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label to one-hot\n",
    "# from keras.utils import np_utils\n",
    "# train_labels = np_utils.to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index = 6240\n",
    "# print(train_arrays[index])\n",
    "# print(train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,Y = shuffle(train_arrays,train_labels,random_state=1)\n",
    "X_train , X_test , y_train,y_test = train_test_split(X,Y,test_size=0.3,stratify =Y) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 6841), (1.0, 6825), (2.0, 6795), (3.0, 6833), (4.0, 6841), (5.0, 6824), (6.0, 6841), (7.0, 6841), (8.0, 6833), (9.0, 6841), (10.0, 6840), (11.0, 6841), (12.0, 6841), (13.0, 6841), (14.0, 6836), (15.0, 6838)]\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "# smote_enn = SMOTEENN(random_state=0)\n",
    "# X_resampled, y_resampled = smote_enn.fit_sample(X, Y)\n",
    "# print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "X_resampled, y_resampled = smote_tomek.fit_sample(X_train, y_train)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now we train a logistic regression classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ensemble\n",
    "gbdt = ensemble.GradientBoostingClassifier(n_estimators=20,subsample=0.7,verbose =1)\n",
    "#gbdt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbdt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=8,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.ensemble as ensemble\n",
    "\n",
    "rf_classifer = sklearn.ensemble.RandomForestClassifier(n_estimators=300, \n",
    "                                                       max_depth=None,\n",
    "                                                       min_samples_split=2,\n",
    "                                                       min_samples_leaf=1,\n",
    "                                                       min_weight_fraction_leaf=0.0,\n",
    "                                                       max_leaf_nodes=None, \n",
    "                                                       min_impurity_decrease=0.0, \n",
    "                                                       min_impurity_split=None,\n",
    "                                                       bootstrap=True,\n",
    "                                                       oob_score=False,\n",
    "                                                       n_jobs=8,\n",
    "                                                       random_state=None, \n",
    "                                                       verbose=1,\n",
    "                                                       warm_start=False,\n",
    "                                                       class_weight=None)\n",
    "rf_classifer.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77472428132344962"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifer.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "        0.0       0.71      0.82      1.00      0.76      0.84      0.68       129\n",
      "        1.0       0.81      0.85      0.96      0.83      0.89      0.77      1744\n",
      "        2.0       0.79      0.78      0.92      0.79      0.85      0.72      2932\n",
      "        3.0       0.72      0.65      0.97      0.68      0.83      0.67      1064\n",
      "        4.0       0.74      0.83      1.00      0.78      0.86      0.72       184\n",
      "        5.0       0.80      0.82      0.97      0.81      0.88      0.77      1333\n",
      "        6.0       0.58      0.48      1.00      0.53      0.76      0.55        54\n",
      "        7.0       0.89      0.82      1.00      0.86      0.94      0.87       502\n",
      "        8.0       0.71      0.72      0.97      0.72      0.83      0.68       985\n",
      "        9.0       0.67      0.40      1.00      0.50      0.82      0.64        10\n",
      "       10.0       0.67      0.73      1.00      0.70      0.82      0.64       139\n",
      "       11.0       0.61      0.53      1.00      0.57      0.78      0.59        97\n",
      "       12.0       0.57      0.44      1.00      0.50      0.76      0.55         9\n",
      "       13.0       0.73      0.61      1.00      0.66      0.85      0.71       105\n",
      "       14.0       0.79      0.78      0.98      0.78      0.88      0.75      1087\n",
      "       15.0       0.74      0.77      0.98      0.76      0.85      0.71       688\n",
      "\n",
      "avg / total       0.77      0.77      0.96      0.77      0.86      0.73     11062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "y_pred = rf_classifer.predict(X_test)\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function classification_report_imbalanced in module imblearn.metrics.classification:\n",
      "\n",
      "classification_report_imbalanced(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, alpha=0.1)\n",
      "    Build a classification report based on metrics used with imbalanced\n",
      "    dataset\n",
      "    \n",
      "    Specific metrics have been proposed to evaluate the classification\n",
      "    performed on imbalanced dataset. This report compiles the\n",
      "    state-of-the-art metrics: precision/recall/specificity, geometric\n",
      "    mean, and index balanced accuracy of the\n",
      "    geometric mean.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : ndarray, shape (n_samples, )\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : ndarray, shape (n_samples, )\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : list, optional\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average.\n",
      "    \n",
      "    target_names : list of strings, optional\n",
      "        Optional display names matching the labels (same order).\n",
      "    \n",
      "    sample_weight : ndarray, shape (n_samples, )\n",
      "        Sample weights.\n",
      "    \n",
      "    digits : int, optional (default=2)\n",
      "        Number of digits for formatting output floating point values\n",
      "    \n",
      "    alpha : float, optional (default=0.1)\n",
      "        Weighting factor.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    report : string\n",
      "        Text summary of the precision, recall, specificity, geometric mean,\n",
      "        and index balanced accuracy.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from imblearn.metrics import classification_report_imbalanced\n",
      "    >>> y_true = [0, 1, 2, 2, 2]\n",
      "    >>> y_pred = [0, 0, 2, 2, 1] # doctest : +NORMALIZE_WHITESPACE\n",
      "    >>> target_names = ['class 0', 'class 1',     'class 2'] # doctest : +NORMALIZE_WHITESPACE\n",
      "    >>> print(classification_report_imbalanced(y_true, y_pred,     target_names=target_names))\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "    <BLANKLINE>\n",
      "        class 0       0.50      1.00      0.75      0.67      0.71      0.48         1\n",
      "        class 1       0.00      0.00      0.75      0.00      0.00      0.00         1\n",
      "        class 2       1.00      0.67      1.00      0.80      0.82      0.69         3\n",
      "    <BLANKLINE>\n",
      "    avg / total       0.70      0.60      0.90      0.61      0.63      0.51         5\n",
      "    <BLANKLINE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classification_report_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/enxie/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_2 to have shape (None, 21) but got array with shape (28249, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-97d3eba49289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1419\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1420\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_2 to have shape (None, 21) but got array with shape (28249, 1)"
     ]
    }
   ],
   "source": [
    "#单层 神经网络  73%\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=150, input_dim=feature_size))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(units=21))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True),metrics=[ 'acc'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128,verbose=1,validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/dataset/Mature.txt','r')\n",
    "#f_new = open('data/dataset/Mature.txt','w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for line in f:\n",
    "#     l = line.split(\" \")\n",
    "#     j_new = str(l).replace(\",\" ,\"\").replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"\\\\n\",\"\\n\")\n",
    "#     print(j_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listt = os.listdir('./data/dataset')[1:]\n",
    "for i in listt:\n",
    "    path = os.path.join(\"./data/dataset\",i)\n",
    "    f = open(path,'r')\n",
    "    f_new = open(path.replace(\".txt\",\"\")+\"_new.txt\",'w')\n",
    "    for j in f:\n",
    "        j=j.lower().replace(\",\",\" \").replace(\"'\",\" \").replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "        #提取词干\n",
    "#         js = j.split(\" \")\n",
    "#         js_new = [s.stem(i) for i in js]\n",
    "#         j_new = str(js_new).replace(\",\" ,\"\").replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"\\\\n\",\"\\n\")\n",
    "        #j_new=j_new.lower().replace(\",\",\" \").replace(\"'\",\" \").replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "        f_new.write(j)\n",
    "    f.close()\n",
    "    f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
